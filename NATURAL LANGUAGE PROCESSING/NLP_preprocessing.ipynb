{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHzf7wMo7993"
      },
      "source": [
        "# Sentiment Analysis: Data preprocessing\n",
        "\n",
        "In this notebook we are going to compare different approaches of data preprocessing applied to a real world task, analysing movie reviews given by IMBD users. Each review can be classified in two different classes, positive, if the user likes the movie and negative otherwise. This tutorial is inspired by: https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emcDdxME79i5"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Our first step will be to load and prepare all the our data to perform the experiments. In this case we are going to employ IMBD reviews data available at The Training Dataset used is stored in the zipped folder: aclImbdb.tar file. This can also be downloaded from: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "The dataset consists in 50.000 sentences splitted in two sets, train and test of 25.000 sentences each.\n",
        "\n",
        "For our experiments the corpus will be splitted in the following way:\n",
        "* Train_split: 17500 sentences extracted from the original training data.\n",
        "* Validation split: 7500 sentences extracted from the original training data.\n",
        "* Test split: 25000 sentences that form the original test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdPNvE6578X3"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "TEXT = data.Field()\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "SEED = 0\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT,LABEL)\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
        "\n",
        "train_sents = [vars(train_data.examples[i])['text'] for i in range(len(train_data))]    \n",
        "valid_sents = [vars(valid_data.examples[i])['text'] for i in range(len(valid_data))]\n",
        "test_sents =  [ vars(test_data.examples[i])['text'] for i in range(len(test_data))]\n",
        "    \n",
        "train_targets = [1 if vars(train_data.examples[i])['label'] == 'pos' else 0 for i in range(len(train_data))]\n",
        "test_targets = [1 if vars(test_data.examples[i])['label'] == 'pos' else 0 for i in range(len(test_data))]\n",
        "valid_targets = [1 if vars(valid_data.examples[i])['label'] == 'pos' else 0 for i in range(len(valid_data))]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN3NNt_G78Pw"
      },
      "source": [
        "In order to feed our data into a model that is able to predict the sentiment of our movie reviews we should first create a vocabulary of all the words that appear in our training data. \n",
        "\n",
        "A measure of quality of our vocabulary can be its coverage over the data. We can define the vocabulary coverage as the percentage of tokens from our data that are found in our vocabulary. \n",
        "\n",
        "As we will see later, vocabulary size is a paremeter that can be tuned and this coverage can serve as guidande.\n",
        "\n",
        "**Compute the vocabulary as a dictionary wirh words as keys and the number of times the word appears as value.**\n",
        "\n",
        "**Compute the coverage over the data given our vocabulary.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYSHNxa078HT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a55923b-392e-41e8-e1b5-dcfa379ecfae"
      },
      "source": [
        "def compute_vocabulary(train_sents):\n",
        "  vocabulary = {'<unk>':99999999}\n",
        "  for sent in train_sents:\n",
        "    for token in sent:\n",
        "      if token in vocabulary:\n",
        "        vocabulary[token] += 1\n",
        "      else:\n",
        "        vocabulary[token] = 1\n",
        "      \n",
        "  return vocabulary\n",
        "\n",
        "def coverage(split,voc):\n",
        "  total = 0.0\n",
        "  unk = 0.0\n",
        "  for sent in split:\n",
        "    for token in sent:\n",
        "      if not  token in voc:\n",
        "          unk += 1.0\n",
        "      total += 1.0\n",
        "  return 1.0 - (unk/total)\n",
        "\n",
        "\n",
        "def voc_stats(split,voc):\n",
        "  print('**** VOCABULARY ***')\n",
        "  print('* Unique words', len(voc))\n",
        "  print('* Coverage', coverage(split,voc))\n",
        "\n",
        "voc = compute_vocabulary(train_sents)\n",
        "voc_stats(valid_sents, voc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 224384\n",
            "* Coverage 0.9638441717742088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OncZryV877-b"
      },
      "source": [
        "Our first proposed preprocess is word level tokenization. In this cases all word will be splitted and stop-words will be separated. In this case we are going to employ 'nltk' library for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j9vfkqo771Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d46c350-8124-4989-adc3-3e7168999a51"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer    \n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "stop_words = list(stopwords.words('english')) #About 150 stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_sentence(sentence):\n",
        "    return  nltk.word_tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlEBwZY977sO"
      },
      "source": [
        "**Compute the tokenization and vocabulary given the function avobe. How the vocabulary size and the coverage have changed?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KpDN5d_77ie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fa3a71-73ea-4315-db61-4e2816f05e6f"
      },
      "source": [
        "train_tok_sents = [tokenize_sentence(' '.join(s)) for s in train_sents]\n",
        "valid_tok_sents = [tokenize_sentence(' '.join(s)) for s in valid_sents]\n",
        "test_tok_sents = [tokenize_sentence(' '.join(s)) for s in test_sents]\n",
        "\n",
        "tok_voc = compute_vocabulary(train_tok_sents)\n",
        "voc_stats(valid_tok_sents, tok_voc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 111358\n",
            "* Coverage 0.9867840235348441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbvTTK-V77Zu"
      },
      "source": [
        "\n",
        "In addition to the previous steps we are going further preprocess our data.\n",
        "\n",
        "For each review in the corpus we apply a preprocess consisting in:\n",
        "\n",
        "- Each word is replaced by its lemma form. Lemmatization reduces vocabulary size as all different forms of a word are grouped in a single lemma.\n",
        "- Remove casing from words, this helps reduce ambiguity as upper and lower cased appearences of a word are selected as different in the vocabulary.\n",
        "- Remove stop words. In order to reduce sentence length and represent the words that carry the meaning of the sentence, we remove all stop words.\n",
        "\n",
        "**Apply the preprocess and compute its new vocabulary. How did stats change?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7MVF0vK77QK"
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    return  [lemmatizer.lemmatize(word.lower()) for word in sentence if not word in stop_words]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-r8qFxn77F_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a30f49d-c283-4af2-b7c1-60294fd2f4ef"
      },
      "source": [
        "train_tok_sents = [preprocess_sentence(s) for s in train_tok_sents]\n",
        "valid_tok_sents = [preprocess_sentence(s) for s in valid_tok_sents]\n",
        "test_tok_sents = [preprocess_sentence(s) for s in test_tok_sents]\n",
        "\n",
        "tok_voc = compute_vocabulary(train_tok_sents)\n",
        "voc_stats(valid_tok_sents, tok_voc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 88081\n",
            "* Coverage 0.9838491774231946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmDMCuz8768V"
      },
      "source": [
        "In both cases vocabularies are too big to be handled in this task as a lot of words only appear a few times in the dataset of some ambiguity is still present in the data.\n",
        "\n",
        "**Create a 5000 token vocabulary that only include the most frequent tokens in the dataset. How is the new coverage?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7ZLjZFl76zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d938392b-4df1-4c6f-fe32-bc3000daa0a0"
      },
      "source": [
        "def reduce_vocabulary(voc,size=5000):\n",
        "  items = list(voc.items())\n",
        "  items.sort(key=lambda x: x[1], reverse=True)\n",
        "  items = items[:size]\n",
        "  return {k:v for k,v in items}\n",
        "\n",
        "SIZE = 5000\n",
        "\n",
        "tok_voc = reduce_vocabulary(tok_voc,size=SIZE)\n",
        "voc = reduce_vocabulary(tok_voc,size=SIZE)\n",
        "voc_stats(valid_tok_sents,tok_voc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 5000\n",
            "* Coverage 0.8758908085582482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxVXR-DW76rW"
      },
      "source": [
        "The previous preprocess helps to reduce the ambiguity produced by the different\n",
        "form of the same word or stop-words included in other tokens, but the original tokens are mostly unchanged. In the following cases we will explore other levels of tokenization, characters and subwords. \n",
        "\n",
        "Let's start with characters, where all words in the dataset are stripped into its individual characters, which has several new characterisitics:\n",
        "* Vocabularies are orders of magnitude smaller that its word counterparts, as all words share the same script that uses a limited set of them.\n",
        "* Each token does not provide a lot of information about the sentence. Individual words include semantic and morpghological information.\n",
        "* Character's use is more ambiguious, words are used generally on the same context consistently during the dataset while characters can belong to a great nunmber of words.\n",
        "\n",
        "**Based on the previous tokenization compute a character vocabulary over the training, and measure its size and coverage of the validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l2ifQu576hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23152e0c-8b1b-4b80-a10f-f9f2b884f818"
      },
      "source": [
        "train_char_sents = [' '.join(s).strip() for s in train_tok_sents]\n",
        "valid_char_sents = [' '.join(s).strip() for s in valid_tok_sents]\n",
        "test_char_sents = [' '.join(s).strip() for s in test_tok_sents]\n",
        "\n",
        "char_voc = compute_vocabulary(train_char_sents)\n",
        "voc_stats(valid_char_sents,char_voc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 130\n",
            "* Coverage 0.9999934164930838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMUgsWe_76V_"
      },
      "source": [
        "Character level may be too extreme for some tasks, but it provides a great coverage of the dataset. In those cases a great alternative e to apply is subword tokenization. \n",
        "\n",
        "In this case words are splitted in pieces which lenght depends on how common they are in the dataset. This way long pieces that are really common will be maintained, for example lemmas of common words in the data, while for not common words or affixes the vocabulary includes smaller pieces until arriving to  individual characters.\n",
        "\n",
        "This tokenizations allows:\n",
        "* A great coverage of the data, as only words including characters not present in the training data will not be recognized.\n",
        "* A parametrized vocabulary size. The number of tokens in the vocabulary is set when the tokenization is computed and can be tuned to improve the models performance.\n",
        "\n",
        "The example shows a call to Byte-Pair-Encoding tokenization using the standard subword-nmt library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVZde6La76Jn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22459fc7-8b0d-4d45-8421-21ebe796c6e5"
      },
      "source": [
        "! pip install subword-nmt\n",
        "\n",
        "# Write the data splits into files to compute the Byte-Pair Encoding (BPE)\n",
        "with open('train.txt','w+') as tr:\n",
        "  for s in train_tok_sents:\n",
        "    print(' '.join(s),file=tr)\n",
        "\n",
        "with open('valid.txt','w+') as vl:\n",
        "  for s in valid_tok_sents:\n",
        "    print(' '.join(s),file=vl)\n",
        "\n",
        "with open('test.txt','w+') as ts:\n",
        "  for s in test_tok_sents:\n",
        "    print(' '.join(s),file=ts)\n",
        "\n",
        "\n",
        "# Compute BPE codes and apply to the data splits\n",
        "! subword-nmt learn-bpe -s 5000 < train.txt > bpe5000.codes\n",
        "! subword-nmt apply-bpe -c bpe5000.codes < train.txt > train.bpe.txt\n",
        "! subword-nmt apply-bpe -c bpe5000.codes < valid.txt > valid.bpe.txt\n",
        "! subword-nmt apply-bpe -c bpe5000.codes < test.txt > test.bpe.txt\n",
        "\n",
        "train_bpe_sents = []\n",
        "with open('train.bpe.txt') as tr:\n",
        "  for line in tr.readlines():\n",
        "    train_bpe_sents.append(line.replace('\\n','').split())\n",
        "\n",
        "  valid_bpe_sents = []\n",
        "with open('valid.bpe.txt') as tr:\n",
        "  for line in tr.readlines():\n",
        "    valid_bpe_sents.append(line.replace('\\n','').split())\n",
        "\n",
        "test_bpe_sents = []\n",
        "with open('test.bpe.txt') as tr:\n",
        "  for line in tr.readlines():\n",
        "    test_bpe_sents.append(line.replace('\\n','').split())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Installing collected packages: subword-nmt\n",
            "Successfully installed subword-nmt-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMlq8XJU75_o"
      },
      "source": [
        "**Compute the coverage and vocabulary size of the subword tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt6QCC_Vpp6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacbfba7-0174-4698-a895-501a17bd3a6d"
      },
      "source": [
        "bpe_voc = compute_vocabulary(train_bpe_sents)\n",
        "voc_stats(valid_bpe_sents,bpe_voc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 5196\n",
            "* Coverage 0.9999679559433573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw2BHcot75rj"
      },
      "source": [
        "# Classification Task\n",
        "\n",
        "In this final section we are going to measure the performance of a model when using as features the different tokenization levels described above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGM2drNf75hE"
      },
      "source": [
        "The first step in order to train a model is decide how to feed our data into the model. For these experiments, we are going to employ bag of words vectors as they do not require any further preprocess.\n",
        "\n",
        "Bag of Words (BOW) consists in representing each of the sentences of the dataset as fixed size vectors of the size of the vocabulary. For example, for our 5000 tokens vocabularies, each sentence will be represented as a 5000 dimension vectors, following these steps:\n",
        "\n",
        "- Initially all vector dimensions are initialized as 0.\n",
        "- For each word in the sentence, 1 is added to the position of such word in the vocabulary.\n",
        "\n",
        "The final vector is a sparce vector, most of its values are 0, that contains the number of times each word appears in the sentence and the sum of all dimensions of the vector is the number of tokens in the sentence.\n",
        "\n",
        "Note, that while this representation provides a fixed size representation of the sentences it lacks information of the order in which words appear.\n",
        "\n",
        "**Create a general model to compute the vocabulary indexes and the BOW representations of our data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHhUM1xD75WK"
      },
      "source": [
        "def idx_voc(voc):\n",
        "  items = list(voc.items())\n",
        "  items.sort(key=lambda x: x[1], reverse=True)\n",
        "  return {item[0]:n for n,item in enumerate(items)}\n",
        "\n",
        "def bag_of_words(splits,voc):\n",
        "  voc = idx_voc(voc)\n",
        "  bow_splits = []\n",
        "  for split in splits:\n",
        "    bow = []\n",
        "    for sent in split:\n",
        "      vector = [0] * len(voc)\n",
        "      for s in sent:\n",
        "        try:\n",
        "          vector[voc[s]] += 1\n",
        "        except:\n",
        "          vector[voc['<unk>']] += 1\n",
        "      bow.append(vector)\n",
        "    bow_splits.append(bow)\n",
        "  return bow_splits\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvQKOit775Kj"
      },
      "source": [
        "As a classifier we are going to employ Random forest (https://towardsdatascience.com/understanding-random-forest-58381e0602d2), a model that consists in an ensemble of Decision Trees that see different data by means of bagging and boosting. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AjHmPMk74_o"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "def get_accuracy(preds,labels):\n",
        "    return sum([p == l for p,l in zip(preds,labels)])/len(preds)*100\n",
        "\n",
        "def trainRandomForestClassifier(train_idx,test_idx):\n",
        "  train_idx = np.array(train_idx,dtype='float')\n",
        "  test_idx = np.array(test_idx,dtype='float')\n",
        "\n",
        "  clf = RandomForestClassifier(n_estimators=100, max_depth=3,random_state=SEED)\n",
        "  clf.fit(train_idx,train_targets)\n",
        "\n",
        "  preds = clf.predict(test_idx)\n",
        "  print('Accuracy', get_accuracy(preds,test_targets),'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBe3wqQ-74zB"
      },
      "source": [
        "**Train the model for the different preprocesses. How they affect the results?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhZLpbEA74n0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "587b929e-485d-4988-ca44-50a29d58cbe7"
      },
      "source": [
        "# Tokenized at word level and preprocessed data\n",
        "train_idx,valid_idx,test_idx = bag_of_words([train_tok_sents,valid_tok_sents,test_tok_sents],tok_voc)\n",
        "trainRandomForestClassifier(train_idx,test_idx)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 78.488 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPTLrOpxqHYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1689470e-c1e1-4ad6-a22d-a27618a2e7e2"
      },
      "source": [
        "#Tokenized and character data\n",
        "train_idx,valid_idx,test_idx = bag_of_words([train_char_sents,valid_char_sents,test_char_sents],char_voc)\n",
        "trainRandomForestClassifier(train_idx,test_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 60.844 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD8GEgGtqID7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b09a99-6c61-4798-9cf0-346125e133b4"
      },
      "source": [
        "#BPE tokenized data\n",
        "train_idx,valid_idx,test_idx = bag_of_words([train_bpe_sents,valid_bpe_sents,test_bpe_sents],bpe_voc)\n",
        "trainRandomForestClassifier(train_idx,test_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 79.392 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlvrxdXk74cT"
      },
      "source": [
        "# Conclusions\n",
        "* Preprocessing plays an important role in the performance of NLP systems.\n",
        "* Even for the same model performance may vary a lot depending on how expressive are the features selected for the task.\n",
        "* All methods  presented (with the exception of no preprocess) are employed in state of the art work for several tasks.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3fsubjE74P-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}