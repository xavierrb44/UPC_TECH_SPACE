{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Lab 2: Embeddings_solved (slow track).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8EvKKcc-mL0"
      },
      "source": [
        "#Vector Representations of Words\n",
        "\n",
        "This lab is devoted to vector representations of words. We will empirically use and evaluate word embeddings.\n",
        "\n",
        "In NLP systems we need to deal with different kinds of discrete data:\n",
        "* words\n",
        "* characters\n",
        "* part-of-speech tags\n",
        "* named entities\n",
        "* items in a product catalog, and so on. \n",
        "\n",
        "Although words comprise a finite set -- vocabulary -- representing such discrete types as dense vectors is challenging and has a great impact on the quality of the overall NLP system. \n",
        "\n",
        "**Representation learning** and **embedding** refer to learning the mapping from one discrete type to a point in the vector space. And therefore, when these discrete types are words, we call these dense vector representation is called a **word embedding**. \n",
        "\n",
        "Today we will explore:\n",
        "1. Count-based embeddings: one-hot vectors, Term-Frequency and Term-Frequency-Inverse-Document-Frequency (TF-IDF). \n",
        "2. Word embeddings: GloVe and Word2Vec.\n",
        "3. How to visualize word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwmChUernDpd"
      },
      "source": [
        "##Prerequisites\n",
        "\n",
        "###Data\n",
        "\n",
        "First, let's download a dataset which will be used to train our embeddings. It is a pretty large set, so let's do it now, so later we won't have to wait for it to finish downloading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XifLsaiuFtFV"
      },
      "source": [
        "!wget -P data/glove http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -d data/glove data/glove/glove.6B.zip\n",
        "\n",
        "glove_path='data/glove/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4O1Y0agf_h2"
      },
      "source": [
        "pip install seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MOjJOTuNEkU"
      },
      "source": [
        "##1. Count-based (frequency-based) Embeddings\n",
        "\n",
        "Traditional methods for creating vector representations of words, are called count-based. For term frequencies in a corpus the basis of the vector space is the vocabulary of the corpus. For example, in a sentence or a document each word is characterised as the number of times it appears there. For this we can construct **occurance matrix**, where a **document vector** is created for each sentence/document and is of size of the number of unique words. On the other hand, the **word vector** is of the size of the number of sentences/documents. The count-based representations are also called distributional representations because their significant content or meaning is represented by multiple dimensions in the vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGDBo-IaVVUb"
      },
      "source": [
        "###One-hot vectors and TF representations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK09bnjJqLGG"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import seaborn as sns\n",
        "\n",
        "corpus = ['This cell phone is very small and cheap.', 'This phone is great and works great for a small phone']\n",
        "one_hot_vectorizer = CountVectorizer(analyzer='word', binary=True)\n",
        "one_hot = one_hot_vectorizer.fit_transform(corpus)\n",
        "vocab = one_hot_vectorizer.get_feature_names()\n",
        "\n",
        "sns.heatmap(one_hot.toarray(), annot=True, cbar=False, xticklabels=vocab, yticklabels=['Sentence 1','Sentence 2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hiWgUrpyEOF"
      },
      "source": [
        "The **TF representation** of a phrase, sentence, or document is simply the sum of the one-hot representations of its constituent words. Notice that each entry is a count of the number of times the corresponding word appears in the sentence (corpus)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGaCuBzXxfkI"
      },
      "source": [
        "corpus.append('My cell phone is in a small cell.')\n",
        "### WRITE YOUR CODE HERE ###\n",
        "# to-do:\n",
        "# implement a TF vectorizer that allows to have word frequencies for each sentence. \n",
        "\n",
        "### END OF YOUR CODE ###\n",
        "vector = vectorizer.fit_transform(corpus)\n",
        "vocab = vectorizer.get_feature_names()\n",
        "\n",
        "sns.heatmap(vector.toarray(), annot=True, cbar=False, xticklabels=vocab, yticklabels=['Sentence 1','Sentence 2','Sentence 3'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_mSAaRRg5d-"
      },
      "source": [
        "######SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WITjL-vcg729"
      },
      "source": [
        "corpus.append('My cell phone is in a small cell.')\n",
        "### WRITE YOUR CODE HERE ###\n",
        "# to-do:\n",
        "# implement a TF vectorizer that allows to have word frequencies for each sentence. \n",
        "vectorizer = CountVectorizer(analyzer='word', binary=False)\n",
        "### END OF YOUR CODE ###\n",
        "vector = vectorizer.fit_transform(corpus)\n",
        "vocab = vectorizer.get_feature_names()\n",
        "\n",
        "sns.heatmap(vector.toarray(), annot=True, cbar=False, xticklabels=vocab, yticklabels=['Sentence 1','Sentence 2','Sentence 3'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9VDZg0IyqWM"
      },
      "source": [
        "###TF-IDF\n",
        "The **TF-IDF representation** penalizes common tokens and rewards rare tokens in the vector representation. \n",
        "In deep learning, it is rare to see inputs encoded using heuristic representations like TF-IDF because the goal is to learn a representation. Often, we start with a one-hot encoding using integer indices and a special “embedding lookup” layer to construct inputs to the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwfq9_xtzNcX"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer() \n",
        "tfidf = tfidf_vectorizer.fit_transform(corpus).toarray() \n",
        "\n",
        "sns.heatmap(tfidf, annot=True, cbar=False, xticklabels=vocab, yticklabels= ['Sentence 1', 'Sentence 2', 'Sentence 3'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFdkQNPcNKF9"
      },
      "source": [
        "##2. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ0_E-qBaCqv"
      },
      "source": [
        "Distributed representations enable to represent words as a much lower-dimension dense vector (say d=100, as opposed to the size of the entire vocabulary). Low-dimensional learned dense representations have several benefits over the one-hot and count-based vectors\n",
        "* reducing the dimensionality is computationally efficient\n",
        "* count-based representations result in high-dimensional vectors that redundantly encode similar information along many dimensions\n",
        "* very high dimensions in the input can result in real problems in machine learning and optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIZv6C6sXvYC"
      },
      "source": [
        "###Pre-trained embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX5SE-chpRt-"
      },
      "source": [
        "It is common to use pre-trained embeddings, as they offer good quality embeddings even though they were trained on general (but large!) corpora (e.g. wikipedia). \n",
        "* [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
        "* [fastText](https://fasttext.cc/docs/en/english-vectors.html)\n",
        "\n",
        "You have already downloaded English GloVe word embeddings - *Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): [**glove.6B.zip**](http://nlp.stanford.edu/data/glove.6B.zip)*. The downloaded file was unzipped and put into a subfolder named glove to result in the following file paths: `data/glove/glove.6B.100d.txt` and `data/glove/glove.6B.300d.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dlu1cDhGQB9"
      },
      "source": [
        "!head -n 1 data/glove/glove.6B.100d.txt #print out first line of the file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj3oIS1UjXSD"
      },
      "source": [
        "We will be using [Annoy](https://github.com/spotify/annoy) library for effective nearest neighor search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUMKCVmkjQbW"
      },
      "source": [
        "pip install annoy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25_Dxt0DP5ob"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from annoy import AnnoyIndex\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxVEb-mHsqp0"
      },
      "source": [
        "To efficiently load and process embeddings, we describe a utility class called PreTrainedEmbeddings. The class builds an in-memory index of all the word vectors to facilitate quick lookups and nearest-neighbor queries using an approximate nearest-neighbor package, annoy.\n",
        "In these examples, we use the GloVe word embeddings. After you download them, you can instantiate with the PretrainedEmbeddings class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGyLBDIRkI3e"
      },
      "source": [
        "####Loading embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCivOjJRkhl6"
      },
      "source": [
        "class PreTrainedEmbeddings(object):\n",
        "    def __init__(self, word_to_index, word_vectors):\n",
        "        self.word_to_index = word_to_index\n",
        "        self.word_vectors = word_vectors\n",
        "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
        "\n",
        "        self.index = AnnoyIndex(len(word_vectors[0]), metric='euclidean')\n",
        "        print(\"Building Index!\")\n",
        "        for _, i in self.word_to_index.items():\n",
        "            self.index.add_item(i, self.word_vectors[i])\n",
        "        self.index.build(50)\n",
        "        print(\"Finished!\")\n",
        "        \n",
        "    @classmethod\n",
        "    def from_embeddings_file(cls, embedding_file):\n",
        "        word_to_index = {}\n",
        "        word_vectors = []\n",
        "\n",
        "        with open(embedding_file) as fp:\n",
        "            ### WRITE YOUR CODE HERE ###\n",
        "            # to-do:\n",
        "            # write a loop to read the file with vectors line by line\n",
        "            # store each word with its corresponding vector (as a numpy array)\n",
        "            # implement word_to_index which is a dict mapping words to integers\n",
        "            # implement word_vectors which is a list of numpy arrays\n",
        "            ### END OF YOUR CODE ###\n",
        "        return cls(word_to_index, word_vectors)\n",
        "    \n",
        "    def get_embedding(self, word):\n",
        "        ### WRITE YOUR CODE HERE ###\n",
        "        # to-do:\n",
        "        # return word_vectors\n",
        "\n",
        "        ### END OF YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi5DZzahi0BF"
      },
      "source": [
        "######SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8ucsmKhjyN1"
      },
      "source": [
        "class PreTrainedEmbeddings(object):\n",
        "    def __init__(self, word_to_index, word_vectors):\n",
        "        self.word_to_index = word_to_index\n",
        "        self.word_vectors = word_vectors\n",
        "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
        "\n",
        "        self.index = AnnoyIndex(len(word_vectors[0]), metric='euclidean')\n",
        "        print(\"Building Index!\")\n",
        "        for _, i in self.word_to_index.items():\n",
        "            self.index.add_item(i, self.word_vectors[i])\n",
        "        self.index.build(50)\n",
        "        print(\"Finished!\")\n",
        "        \n",
        "    @classmethod\n",
        "    def from_embeddings_file(cls, embedding_file):\n",
        "        word_to_index = {}\n",
        "        word_vectors = []\n",
        "\n",
        "        with open(embedding_file) as fp:\n",
        "            ### WRITE YOUR CODE HERE ###\n",
        "            # to-do:\n",
        "            # write a loop to read the file with vectors line by line\n",
        "            # store each word with its corresponding vector (as a numpy array)\n",
        "            # implement word_to_index which is a dict mapping words to integers\n",
        "            # implement word_vectors which is a list of numpy arrays\n",
        "            for line in fp.readlines():\n",
        "                line = line.split(\" \")\n",
        "                word = line[0]\n",
        "                vec = np.array([float(x) for x in line[1:]])\n",
        "                \n",
        "                word_to_index[word] = len(word_to_index)\n",
        "                word_vectors.append(vec)\n",
        "            ### END OF YOUR CODE ###\n",
        "        return cls(word_to_index, word_vectors)\n",
        "    \n",
        "    def get_embedding(self, word):\n",
        "        ### WRITE YOUR CODE HERE ###\n",
        "        # to-do:\n",
        "        # return word_vectors\n",
        "        return self.word_vectors[self.word_to_index[word]]\n",
        "        ### END OF YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HOXxpTxV3Ht"
      },
      "source": [
        "####Retrieving vectors of words\n",
        "Load the pre-trained embeddings and check what is the vector for a given word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1JFEU6zV4EY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "8bbdaa1e-fde9-40b5-8375-46b54f3234dc"
      },
      "source": [
        "embeddings = PreTrainedEmbeddings.from_embeddings_file(glove_path + 'glove.6B.300d.txt')\n",
        "\n",
        "word_exp = 'the'\n",
        "print(\"word: {} \\nvector: \\n{}\".format(word_exp, embeddings.get_embedding(word_exp)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Index!\n",
            "Finished!\n",
            "word: the \n",
            "vector: \n",
            "[ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
            " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
            "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
            " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
            " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
            "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
            "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
            " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
            " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
            "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
            " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
            "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
            "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
            " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
            "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
            " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
            " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
            "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
            "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
            "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
            "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
            "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
            "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
            " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
            " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
            "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
            " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
            "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
            "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
            " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
            "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
            " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
            " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
            " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
            "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
            "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
            "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
            " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
            " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
            "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
            " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
            "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
            "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
            "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
            " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
            " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
            " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
            " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
            "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
            "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnkDJLqdkdow"
      },
      "source": [
        "####Word Analogy task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srVCzHkCnIcU"
      },
      "source": [
        "Word embeddings is that the encode syntactic and semantic relationships. We can explore the semantic relationships encoded in word embeddings. One of the most popular methods is a word analogy task. In this task, you are provided with the first three words and need to determine the fourth word. Interestingly, the simple word analogy task can demonstrate that word embeddings capture a variety of semantic and syntactic relationships.\n",
        "\n",
        "Now, let us implement Word Analogy logic to test our pre-trained embeddings. We employ geometric properties of word embeddings, for example:\n",
        "\n",
        "**king - man + woman = queen**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoVHsf3-kUT_"
      },
      "source": [
        "def get_closest_to_vector(self, vector, n=1):\n",
        "    nn_indices = self.index.get_nns_by_vector(vector, n)\n",
        "    return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
        "\n",
        "def word_analogy(self, word1, word2, word3):\n",
        "    existing_words = set([word1, word2, word3])\n",
        "    vec1 = self.get_embedding(word1)\n",
        "\n",
        "    ### WRITE YOUR CODE HERE ###\n",
        "    # to-do:\n",
        "    # implement get vectors for vec2, vec3 and vec4\n",
        "    # get closest_words for vec4 (at most 4) and make sure they are not the same as vec1, vec2 or vec3\n",
        "    ### END OF YOUR CODE HERE ###\n",
        "\n",
        "    print_analogy(self, word1, word2, word3, closest_words)\n",
        "\n",
        "def print_analogy(self, word1, word2, word3, closest_words):\n",
        "    if len(closest_words) == 0:\n",
        "        print(\"Could not find nearest neighbors for the computed vector!\")\n",
        "        return        \n",
        "    for word4 in closest_words:\n",
        "        print(\"{} : {} \\t {} : {}\".format(word1, word2, word3, word4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rshClTnllM6v"
      },
      "source": [
        "######SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8p3wzCGlO_E"
      },
      "source": [
        "def get_closest_to_vector(self, vector, n=1):\n",
        "    nn_indices = self.index.get_nns_by_vector(vector, n)\n",
        "    return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
        "\n",
        "def word_analogy(self, word1, word2, word3):\n",
        "    existing_words = set([word1, word2, word3])\n",
        "    vec1 = self.get_embedding(word1)\n",
        "\n",
        "    ### WRITE YOUR CODE HERE ###\n",
        "    # to-do:\n",
        "    # implement get vectors for vec2, vec3 and vec4\n",
        "    # get closest_words for vec4 (at most 4) and make sure they are not the same as vec1, vec2 or vec3\n",
        "    vec2 = self.get_embedding(word2)\n",
        "    vec3 = self.get_embedding(word3)\n",
        "    vec4 = vec2 - vec1 + vec3\n",
        "\n",
        "    closest_words = get_closest_to_vector(self, vec4, n=4)        \n",
        "    closest_words = [word for word in closest_words if word not in existing_words]\n",
        "    ### END OF YOUR CODE HERE ###\n",
        "\n",
        "    print_analogy(self, word1, word2, word3, closest_words)\n",
        "\n",
        "def print_analogy(self, word1, word2, word3, closest_words):\n",
        "    if len(closest_words) == 0:\n",
        "        print(\"Could not find nearest neighbors for the computed vector!\")\n",
        "        return        \n",
        "    for word4 in closest_words:\n",
        "        print(\"{} : {} \\t {} : {}\".format(word1, word2, word3, word4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiZzkH6eWCZ0"
      },
      "source": [
        "#####Examples\n",
        "It is time to perform the word analogy task using our pre-trained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3MEqUi4WCtd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "dfd39b16-e5c3-4c54-b56f-b56ccf5b330e"
      },
      "source": [
        "word_analogy(embeddings, 'man', 'he', 'woman')\n",
        "word_analogy(embeddings, 'cat', 'kitten', 'dog')\n",
        "word_analogy(embeddings, 'car', 'cars', 'bicycle')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man : he \t woman : she\n",
            "man : he \t woman : her\n",
            "cat : kitten \t dog : puppy\n",
            "cat : kitten \t dog : rottweiler\n",
            "cat : kitten \t dog : pooch\n",
            "car : cars \t bicycle : bicycles\n",
            "car : cars \t bicycle : bikes\n",
            "car : cars \t bicycle : bike\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--XzCMg0tt9I"
      },
      "source": [
        "###Training Word2Vec embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prq4gy1vvB_4"
      },
      "source": [
        "However, if we are implementing an NLP system that is designed to operate in a domain where general embeddings won't perform well, then we can train word embeddings using our own corpus.\n",
        "\n",
        "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the data, a list (sentences) of lists (tokens) for a complete corpus. Word2Vec uses all these tokens to internally create a vocabulary\n",
        "\n",
        "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Remember you are training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1K1nBLMCdsZ"
      },
      "source": [
        "####Data\n",
        "The most important thing is data. We'll use five volumes of Game of Thrones from https://github.com/nihitx/game-of-thrones\n",
        "\n",
        "We will download `got.5books.clean.txt` that is already cleaned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfjky1Av8NfV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "744dfa9a-41c6-47d1-8fae-110c654e91b1"
      },
      "source": [
        "file_share_link = \"https://drive.google.com/open?id=1rNp15OFYZiGWvw1wx_TCa3BZoZ6MjvkQ\"\n",
        "file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_share_link[file_share_link.find(\"=\") + 1:]\n",
        "\n",
        "!wget --no-check-certificate \"$file_download_link\" -P \"data/got\" -O \"got.5books.clean.txt\"\n",
        "\n",
        "got_path='data/got/'\n",
        "!mkdir $got_path\n",
        "!mv \"got.5books.clean.txt\" $got_path/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-04 17:14:06--  https://docs.google.com/uc?export=download&id=1rNp15OFYZiGWvw1wx_TCa3BZoZ6MjvkQ\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.113, 74.125.195.139, 74.125.195.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-14-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/971udi2nmj8fagvbinqtlp6039p1257r/1591290825000/10683794948536997497/*/1rNp15OFYZiGWvw1wx_TCa3BZoZ6MjvkQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-06-04 17:14:07--  https://doc-08-14-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/971udi2nmj8fagvbinqtlp6039p1257r/1591290825000/10683794948536997497/*/1rNp15OFYZiGWvw1wx_TCa3BZoZ6MjvkQ?e=download\n",
            "Resolving doc-08-14-docs.googleusercontent.com (doc-08-14-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-08-14-docs.googleusercontent.com (doc-08-14-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘got.5books.clean.txt’\n",
            "\n",
            "s.clean.txt             [  <=>               ]   5.38M  25.7MB/s    in 0.2s    \n",
            "\n",
            "2020-06-04 17:14:07 (25.7 MB/s) - ‘got.5books.clean.txt’ saved [5644913]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E6xHfdpCxf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c3c8998-98eb-4343-8f0f-ac99e5a069b1"
      },
      "source": [
        "dataFile= got_path + \"/got.5books.clean.txt\"\n",
        "\n",
        "with open(dataFile, 'rb') as f:\n",
        "    for line in f:\n",
        "        print(line)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'\"We should start back,\" Gared urged as the woods began to grow dark around them. \"The wildlings are dead.\"\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1XUjT8Zbnkd"
      },
      "source": [
        "####Pre-processing data\n",
        "\n",
        "We can read data into a list so that we can pass this on to the Word2Vec model. We'll pre-process data first using `gensim.utils.simple_preprocess(sentence)` - lowercase tokens, ignore tokens that are too short or too long and return a list of tokens (words). Documentation of this pre-processing method can be found in the official [Gensim documentation](https://radimrehurek.com/gensim/utils.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8c-fQ-ucA1w"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# Write a function `readInput(inputFile)` that reads a file and applies the `simple_preprocess`\n",
        "def readInput(inputFile):\n",
        "    \"\"\"Method to read the input file\"\"\"\n",
        "    ## CODE HERE\n",
        "    \n",
        "\n",
        "# read the tokenized file into a list (sentences) of lists (tokens) named `sentences`\n",
        "sentences = list(readInput(dataFile))\n",
        "\n",
        "# print some examples\n",
        "print(sentences[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4W0dYrFnvmK"
      },
      "source": [
        "######SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPzwlRWonxx7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c59b152d-7328-404f-d07e-a30b2f29a12f"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# Write a function `readInput(inputFile)` that reads a file and applies the `simple_preprocess`\n",
        "def readInput(inputFile):\n",
        "    \"\"\"Method to read the input file\"\"\"\n",
        "    ## CODE HERE\n",
        "    lines = []\n",
        "    with open(inputFile, 'rb') as f:\n",
        "        for line in f:\n",
        "            preproc_line = gensim.utils.simple_preprocess(line)\n",
        "            lines.append(preproc_line)\n",
        "    return lines\n",
        "\n",
        "# read the tokenized file into a list (sentences) of lists (tokens) named `sentences`\n",
        "sentences = list(readInput(dataFile))\n",
        "\n",
        "# print some examples\n",
        "print(sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['we', 'should', 'start', 'back', 'gared', 'urged', 'as', 'the', 'woods', 'began', 'to', 'grow', 'dark', 'around', 'them', 'the', 'wildlings', 'are', 'dead']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eUPCHU2cUam"
      },
      "source": [
        "####Training embeddings\n",
        "\n",
        "Training the model is straightforward, you just create Word2Vec instance and pass the data - a list (sentences) of lists (tokens) for a complete corpus. Word2Vec uses all these tokens to create a vocabulary\n",
        "\n",
        "Once the vocabulary is built, we just need to call `train(...)` to start training the Word2Vec model. Remember you are training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieOtIItGKegP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10183cbe-05c7-42ad-f593-ae3802d38a8d"
      },
      "source": [
        "# Define a basic Word2Vec model (gensim.models.Word2Vec) with CBOW and train (model.train) it on `sentences`\n",
        "w2v_model = gensim.models.Word2Vec(sentences, window=5)\n",
        "w2v_model.train(sentences,total_examples=len(sentences),epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7515102, 10107440)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzsKl4xqL-gX"
      },
      "source": [
        "Several functions allow to explore the results. The `most_similar` function returns the top 10 similar words to a given input word. `similarity` returns the similarity between two words that are present in the vocabulary. `doesnt_match` returns the most dissimilar word with a list of words. Let's play with these functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajnglVvpMD7J"
      },
      "source": [
        "# Chose a word to see the 10 closest words with `most_similar`\n",
        "w1 = \"throne\"\n",
        "w2v_model.wv.most_similar(positive=w1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fzT5mR0MTTe"
      },
      "source": [
        "What happens if the word is not in the vocabulary? We are using a tiny corpus in a specific domain..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1vgwYWPMUPe"
      },
      "source": [
        "# Chose a word that you think does not belong to the corpus to see the 10 closest words\n",
        "## CODE HERE\n",
        "w2 = \"google\"\n",
        "w2v_model.wv.most_similar(positive=w2,topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJpXJy7yMcpH"
      },
      "source": [
        "You can also specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related with `most_similar(positive=w1,negative=w2,topn=n)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL_Qc53hMgjI"
      },
      "source": [
        "# get everything related to some word\n",
        "w3 = [\"pillow\", \"sheet\", \"blanket\", \"bed\"]\n",
        "w4 = [\"dirt\"]\n",
        "w2v_model.wv.most_similar(positive=w3, negative=w4, topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "585N3mfYMqM4"
      },
      "source": [
        "# try what happens without the negative constraint\n",
        "w2v_model.wv.most_similar(positive=w3, topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX1AynTjMm_1"
      },
      "source": [
        "Calculate some similarities now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzW4kH2-Mt8M"
      },
      "source": [
        "# similarity between two different words\n",
        "w2v_model.wv.similarity(\"cushion\", \"pillow\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kublSJp8M0jt"
      },
      "source": [
        "# similarity between two identical words\n",
        "## CODE HERE\n",
        "w2v_model.wv.similarity(\"pillow\", \"pillow\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPIddpogM2fV"
      },
      "source": [
        "# similarity between two unrelated words\n",
        "## CODE HERE\n",
        "w2v_model.wv.similarity(\"pillow\", \"dragon\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u112zHPQMxty"
      },
      "source": [
        "Under the hood, the above three snippets compute the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `dirty` is highly similar to `smelly` but `dirty` is dissimilar to `clean`. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7zLEFPrREw2"
      },
      "source": [
        "You can use Word2Vec to find odd items given a list of items with `doesnt_match`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPWZRdVDRGm-"
      },
      "source": [
        "# Define a list of words and look for the strange word\n",
        "# Which one is the odd one out in this list?\n",
        "w5 = [\"pillow\", \"sheet\", \"queen\", \"blanket\", \"bed\"]\n",
        "w2v_model.wv.doesnt_match(w5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zUunkZ-Ri7O"
      },
      "source": [
        "We can use `gensim` to load pre-trained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VaG9U55FnGE"
      },
      "source": [
        "!wc -l data/glove/glove.6B.100d.txt\n",
        "!(echo 400000$'\\t'100 ; cat 'data/glove/glove.6B.100d.txt') > 'data/glove/glove-gensim.6B.100d.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW7vTyXHRncW"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "gloveModel='data/glove/glove-gensim.6B.100d.txt'\n",
        "model_glove = KeyedVectors.load_word2vec_format(gloveModel, binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZdaYnJCxBqa"
      },
      "source": [
        "# find the similarity between two words. \n",
        "# Use the same examples as before an also some examples with out-of-domain vocabulary. I'm sure the word \"phone\" was\n",
        "# not in the vocabulary before!\n",
        "## CODE HERE\n",
        "\n",
        "# get everything related to some word\n",
        "## CODE HERE\n",
        "\n",
        "# The famous (king - man) + woman\n",
        "## CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JUjJdmBw-px"
      },
      "source": [
        "######SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVMqP9c-FEUK"
      },
      "source": [
        "# find the similarity between two words. \n",
        "# Use the same examples as before an also some examples with out-of-domain vocabulary. I'm sure the word \"phone\" was\n",
        "# not in the vocabulary before!\n",
        "\n",
        "## CODE HERE\n",
        "model_glove.wv.similarity(\"cushion\", \"pillow\")\n",
        "\n",
        "# get everything related to some word\n",
        "## CODE HERE\n",
        "w3 = [\"pillow\", \"sheet\", \"blanket\", \"bed\"]\n",
        "w4 = [\"dirt\"]\n",
        "model_glove.wv.most_similar(positive=w3, negative=w4, topn=10)\n",
        "\n",
        "# The famous (king - man) + woman\n",
        "## CODE HERE\n",
        "model_glove.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXVoJNKgP7Wt"
      },
      "source": [
        "##3. Visualization of Word Embeddings\n",
        "\n",
        "Finally, we will visualise the n-dimensional word embeddings by projecting them down to 2-dimensional x,y coordinate pairs. Several techniques exist (PCA, t-SNE, etc). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT44TrQvUrfM"
      },
      "source": [
        "###PCA\n",
        "We use PCA in the following (PCA class in `sklearn.decomposition`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92vqWQit6wan"
      },
      "source": [
        "# Imports needed for the visualisation\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "%matplotlib inline\n",
        "\n",
        "# fit a 2d PCA model to the vectors\n",
        "X = w2v_model[w2v_model.wv.vocab]\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "\n",
        "# add the labels to the plot\n",
        "words = list(w2v_model.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "\n",
        "# look at the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTUIK8-y64oM"
      },
      "source": [
        "Too much information. Let's select only a subset of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPRd3rKn67tP"
      },
      "source": [
        "# Select what we wanna see ('most_similar' words to something for instance)\n",
        "setToPlot = w2v_model.wv.most_similar(positive='throne', topn=10)\n",
        "\n",
        "# Look for the vectors for the desired words only, and store them as vectorX and vectorY\n",
        "vectorX =  []\n",
        "vectorY =  []\n",
        "words = []\n",
        "for word, sim in setToPlot:\n",
        "    i=w2v_model.wv.vocab[word].index\n",
        "    words.append(word)\n",
        "    vectorX.append(result[i,0])\n",
        "    vectorY.append(result[i,1])\n",
        "\n",
        "# create the scatter plot for these words\n",
        "pyplot.scatter(vectorX, vectorY)\n",
        "\n",
        "# add the labels\n",
        "for i, word in enumerate(words):\n",
        "    pyplot.annotate(word, xy=(vectorX[i], vectorY[i]))\n",
        "\n",
        "# look at the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnHtMivwGSpB"
      },
      "source": [
        "#References\n",
        "* This tutorial is partly inspired by the book: \"Natural Language Processing with PyTorch: Build Intelligent Language Applications Using Deep Learning\" by Delip Rao and Brian McMahan."
      ]
    }
  ]
}