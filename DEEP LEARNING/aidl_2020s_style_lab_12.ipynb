{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aidl_2020s_style_lab_12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "M1k8uroWtFiz"
      },
      "source": [
        "# Neural Style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlVgxectH7-m"
      },
      "source": [
        "Lab created by Daniel Fojo for the UPC school 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lis2TOdHHxsz"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nbi4V_eH0Ad"
      },
      "source": [
        "style_weight = 5e3\n",
        "content_weight = 1\n",
        "variation_weight = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8iPaFDwH4V2"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RqdG_NUWZV8"
      },
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hW7k5ysEtFi4"
      },
      "source": [
        "In this session we will use the approach described in [this paper](https://arxiv.org/abs/1508.06576) to apply a particular style to a content image by means of optimization using the weights of a pretrained convnet (VGG16)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ou3KQADKCSf"
      },
      "source": [
        "# Download images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "p5qmPUXOtFjL"
      },
      "source": [
        "Let's first display the images we chose. We want to transfer the style of the second image into the first one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "v_tWKcc8tFjN"
      },
      "source": [
        "from PIL import Image\n",
        "# We get the images from the following repository\n",
        "!wget https://cdn.archpaper.com/wp-content/uploads/2020/06/maarten-van-den-heuvel-gZXx8lKAb7Y-unsplash.jpg -O base.jpg\n",
        "!wget https://images.fineartamerica.com/images/artworkimages/mediumlarge/3/starry-night-print-by-vincent-van-gogh-vincent-van-gogh.jpg -O style.jpg\n",
        "base_image_path = 'base.jpg'\n",
        "style_reference_image_path = 'style.jpg'\n",
        "\n",
        "base_image = Image.open(base_image_path)\n",
        "style_reference_image = Image.open(style_reference_image_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8vpqHhzUk-G"
      },
      "source": [
        "f, axarr = plt.subplots(1,2,figsize=(20,20))\n",
        "axarr[0].imshow(np.asarray(base_image))\n",
        "axarr[0].axis('off')\n",
        "axarr[1].imshow(np.asarray(style_reference_image))\n",
        "axarr[1].axis('off')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9Ne9gbBKGH4"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "vaRJHEUKtFjc"
      },
      "source": [
        "Load VGG and get a dictionary of all its layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29qhDxVaVw1b"
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "model = models.vgg16(pretrained=True).features.to(device).eval()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "FSvoNqEdtFjj"
      },
      "source": [
        "Here we choose the layers of the network that we want to use to represent the style and the content. You can play with different combinations if you wish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A-IlkdNWLBV"
      },
      "source": [
        "content_layers = [20]\n",
        "\n",
        "style_layers = [0, 5, 10, 17, 24]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPw8YGQU6leq"
      },
      "source": [
        "## Exercise 1\n",
        "Complete the code to have a FeaturesExtractor module that returns 2 lists, one with the content and one with the style features.\n",
        "\n",
        "**Hint:** You can use `for i, layer in enumerate(self.base_model):` to iterate the layers of the model in order one by one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5_LgKyIXQSs"
      },
      "source": [
        "import torch.nn as nn\n",
        "class FeaturesExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self, base_model, content_layers, style_layers):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.content_layers = content_layers\n",
        "        self.style_layers = style_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        content_output = []\n",
        "        style_output = []\n",
        "        ...\n",
        "        return content_output, style_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnwkAzyDGSpV"
      },
      "source": [
        "We don't need to compute the gradients of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-tCosWNX-6S"
      },
      "source": [
        "feature_extractor = FeaturesExtractor(model, content_layers, style_layers)\n",
        "for param in feature_extractor.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfvRBL7GKKe5"
      },
      "source": [
        "# Loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k3brM1FGXr1"
      },
      "source": [
        "## Exercise 2\n",
        "Complete the content loss and the style loss.\n",
        "\n",
        "**Hint:** Remember that the content loss is Mean Square Error for the features, and the style loss is Mean Square Error for the Gram matrices of the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36gT4XPbbdfY"
      },
      "source": [
        "content_loss = ...\n",
        "\n",
        "def gram_matrix(x):\n",
        "    b, c, h, w = x.shape\n",
        "    x = x.reshape(b, c, h*w)\n",
        "    gram = torch.bmm(x, x.transpose(1, 2))/(h*w)\n",
        "    return gram\n",
        "\n",
        "def style_loss(style, combination):\n",
        "    ...\n",
        "    return F.mse_loss(S, C)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg1kD_irNVRy"
      },
      "source": [
        "## Extra: Total variation loss\n",
        "\n",
        "Adding the total variation loss to the training loop can help a bit in maintaining local coherence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2quUPwgxNeQp"
      },
      "source": [
        "def total_variation_loss(img):      \n",
        "    tv_h = ((img[:,:,1:,:] - img[:,:,:-1,:]).pow(2)).mean()\n",
        "    tv_w = ((img[:,:,:,1:] - img[:,:,:,:-1]).pow(2)).mean()    \n",
        "    return tv_h + tv_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EfIh_jsKOJA"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLT30l3vGtgG"
      },
      "source": [
        "We will now prepare the data for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u98PgLqbU3qT"
      },
      "source": [
        "img_size = 512\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                                transforms.Resize(img_size),\n",
        "                                transforms.CenterCrop(img_size),\n",
        "                                transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "base_tensor = transform(base_image).to(device).unsqueeze(0)\n",
        "style_reference_tensor = transform(style_reference_image).to(device).unsqueeze(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31eUFdhvHHAo"
      },
      "source": [
        "## Exercise 3\n",
        "Create the `combination_tensor` with the same initial values as base_tensor. It will be the image that we will \"train\". Then, use `optim.LBFGS` as an optimizer to train it, with the default value for lr.\n",
        "\n",
        "**Hint:** You can copy another tensor by using `.clone()`. Remember that trainable tensors have to have `requires_grad=True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUf3DFH7Gz_H"
      },
      "source": [
        "combination_tensor = ...\n",
        "...\n",
        "\n",
        "optimizer = ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_VAhCfMatFjt"
      },
      "source": [
        "# Extract target features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3Z2IuhGG9mv"
      },
      "source": [
        "content_target, _ = feature_extractor(base_tensor)\n",
        "_, style_target = feature_extractor(style_reference_tensor)\n",
        "content_target = [ct.detach() for ct in content_target]\n",
        "style_target = [st.detach() for st in style_target]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dL53PL4HAJI"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLB_L9ZXJSjC"
      },
      "source": [
        "## Exercise 4\n",
        "\n",
        "Complete the training loop. \n",
        "\n",
        "Take into account that the optimizer LBFGS requires a closure, which is a function that sets the gradients to 0, evaluates the model with the current data, computes the loss, computes the gradients and then returns the current loss. Remember to multiply each loss times the corresponding weight defined in the hyperparameters cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiDH9Iu5W-K-"
      },
      "source": [
        "\n",
        "images_history = []\n",
        "\n",
        "for i in range(iterations):\n",
        "\n",
        "    def closure():\n",
        "        # Reset gradients\n",
        "        ...\n",
        "        loss = 0.0\n",
        "\n",
        "        content_output, style_output = feature_extractor(combination_tensor)\n",
        "\n",
        "        # Content loss\n",
        "        cl = 0\n",
        "        for orignal_feature, combination_feature in zip(content_target, content_output):\n",
        "            cl += ...\n",
        "        loss += cl / len(content_layers)\n",
        "        \n",
        "        # Style loss\n",
        "        sl = 0\n",
        "        for orignal_feature, combination_feature in zip(style_target, style_output):\n",
        "            sl += ...\n",
        "        loss +=  sl / len(style_layers)\n",
        "\n",
        "        # Variation loss\n",
        "        vl = ...\n",
        "        loss += vl\n",
        "\n",
        "        if i % 2 == 0:\n",
        "            print(f\"[{i}] Content loss: {cl.item():.2f}, Style loss: {sl.item():.2f}, Variation loss: {vl.item():.2f}\")\n",
        "\n",
        "        # Compute gradients\n",
        "        ...\n",
        "        return loss\n",
        "    \n",
        "    optimizer.step(closure)\n",
        "    images_history.append(combination_tensor.clone().cpu().squeeze().detach().clamp(0, 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH71taFgjnkc"
      },
      "source": [
        "images_history = [transforms.functional.to_pil_image(im) for im in images_history]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZf4BJpD6Gbv"
      },
      "source": [
        "images_history[-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtNKeKyZuZ3k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}