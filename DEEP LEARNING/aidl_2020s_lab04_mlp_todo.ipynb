{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aidl_2020s_lab04_mlp_todo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRxVIQPrHF40"
      },
      "source": [
        "# Image Classification with a Multi-Layer Perceptron\n",
        "\n",
        "**Notebook created in PyTorch by [Santi Pascual](https://github.com/santi-pdp) for the [UPC School](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) (2019).**\n",
        "\n",
        "Based on an original version in Keras created by [Miriam Bellver](https://imatge.upc.edu/web/people/miriam-bellver) for the [Barcelona Technology School](https://barcelonatechnologyschool.com/master/master-in-big-data-solutions/) (BTS) in 2018, and updated by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) and [Xavier Giro](https://imatge.upc.edu/web/people/xavier-giro) in 2019."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNpCbxRWsgM5"
      },
      "source": [
        "The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been around for almost as long as the field itself and has been very intensively studied. It's a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of \"solving\" MNIST as the \"Hello World\" of deep learning -- it's what you do to verify that your algorithms are working as expected. As you become a machine learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on.\n",
        "\n",
        "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc0o6oQ_HHmM"
      },
      "source": [
        "# Set the random seed 123 from numpy\n",
        "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.seed.html\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw1gOr4hIL7F"
      },
      "source": [
        "\n",
        "train_images and train_labels form the \"training set\", the data that the model will learn from. The model will then be tested on the \"test set\", test_images and test_labels. Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqOR_3pBlY6U"
      },
      "source": [
        "### Defining the Hyper-parameters\n",
        "\n",
        "We now define the hyperparameters that are going to be used throughout the notebook\n",
        "to define the network, the data `batch_size`, the training `learning_rate`, and others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR4-Ml6VbDbu"
      },
      "source": [
        "# Let's define some hyper-parameters\n",
        "hparams = {\n",
        "    'batch_size':64,\n",
        "    'num_epochs':10,\n",
        "    'test_batch_size':64,\n",
        "    'hidden_size':128,\n",
        "    'num_classes':10,\n",
        "    'num_inputs':784,\n",
        "    'learning_rate':1e-3,\n",
        "    'log_interval':100,\n",
        "}\n",
        "\n",
        "# we select to work on GPU if it is available in the machine, otherwise\n",
        "# will run on CPU\n",
        "hparams['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# whenever we send something to the selected device (X.to(device)) we already use\n",
        "# either CPU or CUDA (GPU). Importantly...\n",
        "# The .to() operation is in-place for nn.Module's, so network.to(device) suffices\n",
        "# The .to() operation is NOT in.place for tensors, so we must assign the result\n",
        "# to some tensor, like: X = X.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQPfwo_hl99c"
      },
      "source": [
        "### Defining the PyTorch Dataset and the DataLoader\n",
        "\n",
        "The PyTorch Dataset (https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is an inheritable `class` that helps us defining what source of data do we have (image, audio, text, ...) and how to load it (overriding the `__getitem__` function). The MNIST dataset is easible accessible from it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wA_BAvOarC1"
      },
      "source": [
        "mnist_trainset = datasets.MNIST('data', train=True, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]))\n",
        "mnist_testset = datasets.MNIST('data', train=False, \n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
        "                               ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    mnist_trainset,\n",
        "    batch_size=hparams['batch_size'], \n",
        "    shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    mnist_testset,\n",
        "    batch_size=hparams['test_batch_size'], \n",
        "    shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qul4A0lgmsYo"
      },
      "source": [
        "# We can retrieve a sample from the dataset by simply indexing it\n",
        "img, label = mnist_trainset[0]\n",
        "print('Img shape: ', img.shape)\n",
        "print('Label: ', label)\n",
        "\n",
        "# Similarly, we can sample a BATCH from the dataloader by running over its iterator\n",
        "iter_ = iter(train_loader)\n",
        "bimg, blabel = next(iter_)\n",
        "print('Batch Img shape: ', bimg.shape)\n",
        "print('Batch Label shape: ', blabel.shape)\n",
        "print('The Batched tensors return a collection of {} grayscale images ({} channel, {} height pixels, {} width pixels)'.format(bimg.shape[0],\n",
        "                                                                                                                              bimg.shape[1],\n",
        "                                                                                                                              bimg.shape[2],\n",
        "                                                                                                                              bimg.shape[3]))\n",
        "print('In the case of the labels, we obtain {} batched integers, one per image'.format(blabel.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tUAIG4Vnxxt"
      },
      "source": [
        "And now let's look at the kind of images we are dealing with:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVhnJ4Iomm6g"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Definition of a Python function that plots a NxN grid of images contained\n",
        "# in the images array\n",
        "def plot_samples(images,N=5):\n",
        "\n",
        "    '''\n",
        "    Plots N**2 randomly selected images from training data in a NxN grid\n",
        "    '''\n",
        "    \n",
        "    # Randomly select NxN images and save them in ps\n",
        "    ps = random.sample(range(0,images.shape[0]), N**2)\n",
        "\n",
        "    # Allocates figure f divided in subplots contained in an NxN axarr\n",
        "    # https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.subplots.html\n",
        "    f, axarr = plt.subplots(N, N)\n",
        "\n",
        "    # Index for the images in ps to be plotted\n",
        "    p = 0\n",
        "    \n",
        "    # Scan the NxN positions of the grid\n",
        "    for i in range(N):\n",
        "        for j in range(N):\n",
        "          \n",
        "            # Load the image pointed by p\n",
        "            im = images[ps[p]]\n",
        "          \n",
        "            # If images are encoded in grayscale \n",
        "            # (a tensor of 3 dimensions: width x height x luma)...\n",
        "            if len(images.shape) == 3:\n",
        "              # ...specify the colormap as grayscale\n",
        "              # https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.imshow.html\n",
        "              axarr[i,j].imshow(im,cmap='gray')            \n",
        "            else:              \n",
        "              # ...no need to specify any color map\n",
        "              axarr[i,j].imshow(im)\n",
        "              \n",
        "            # Remove axis\n",
        "            axarr[i,j].axis('off')\n",
        "            \n",
        "            # Point to the next image from the random selection\n",
        "            p+=1\n",
        "    # Show the plotted figure         \n",
        "    plt.show()\n",
        "\n",
        "# convert the dataloader output tensors from the previous cell to numpy arrays\n",
        "# The channel dimension has to be squeezed in order for matplotlib to work\n",
        "# with grayscale images\n",
        "img = bimg.squeeze(1).data.numpy()\n",
        "plot_samples(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWHtTIa1oHfX"
      },
      "source": [
        "### BAAAAM, we've got some numbers there"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRPemBuAIXfc"
      },
      "source": [
        "# Training a Multi-Layer Perceptron (MLP)\n",
        "\n",
        "Now that we have the dataset loaded and prepared, let's get some deep stuff spinning. \n",
        "\n",
        "Our workflow will be as follow: first we will train our neural network with the training data, loaded from the constructed `train_loader`. The network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for `test_loader` images, and we will verify if these predictions match the labels from `test_loader`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCuGAZaZqDGe"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "For the time being, we will use a very simple network. It consists of a sequence of two `nn.Linear` layers, which are densely-connected (also called \"fully-connected\") neural layers. The last layer is a 10-way `nn.LogSoftmax` layer, which means it will return an array of 10 log-probability scores. Each score will be the probability that the current digit image belongs to one of our 10 digit classes. Please fill in the network definition below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkE8HbX-IY6r"
      },
      "source": [
        "# Define a variable 'network' by instantiating a PyTorch sequential model\n",
        "# https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential\n",
        "\n",
        "# Add a (1) nn.Linear hidden layer with 128 neurons,, (2) a nn.ReLU, (3) the output nn.Linear and,\n",
        "# the (4) output nn.LogSoftmax\n",
        "# NOTE: Consider the 'num_inputs', 'hidden_size', and 'num_classes' parameters \n",
        "# defined above as hyper-params\n",
        "\n",
        "#TODO\n",
        "network=TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZTxMsJFplbr"
      },
      "source": [
        "Now we can check which is the architecture of the network, and the number of parameters of each layer with the following helper function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0PJJLTQpj-l"
      },
      "source": [
        "def get_nn_nparams(net):\n",
        "  \"\"\" https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/6 \"\"\"\n",
        "  pp=0\n",
        "  for p in list(net.parameters()):\n",
        "      nn=1\n",
        "      for s in list(p.size()):\n",
        "          nn = nn*s\n",
        "      pp += nn\n",
        "  return pp\n",
        "  \n",
        "print(network)\n",
        "print('Num params: ', get_nn_nparams(network))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWbAtS9NqqJV"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "To make our network ready for training, we need to pick three more things:\n",
        "\n",
        "*    **A loss function**: this is how the network will be able to measure how good a job works on its training data, and thus how it will be able to steer itself in the right direction. **Check the [PyTorch documentation](https://pytorch.org/docs/stable/nn.functional.html#nll-loss) for the negative log likelihood loss for multi-class classification from the functional API.**\n",
        "*   **An optimizer**: this is the mechanism through which the network will update itself based on the data it sees and its loss function. **Check the PyTorch documentation and find the RMSprop optimizer to use it.**\n",
        "*   **Metrics to monitor during training and testing**. Here we will only care about accuracy (the fraction of the images that were correctly classified). **Define the accuracy function to return, for a batch, the count of correct predictions (hence same prediction as the label).**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CBYSAGqp6km"
      },
      "source": [
        "# Import optmizer \n",
        "# https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop\n",
        "import torch.optim as optim\n",
        "\n",
        "# Import a functional API for the loss function (use this one !!)\n",
        "# https://pytorch.org/docs/stable/nn.functional.html#nll-loss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Optimizer: RMS Prop (use the hparams['learning_rate'] previously defined)\n",
        "optimizer=TODO\n",
        "\n",
        "# Negative Log Likelihood (NLL) Loss criterion from the functional API\n",
        "criterion=TODO\n",
        "\n",
        "# Define the Accuracy metric in the function below by:\n",
        "  # (1) obtain the maximum for each predicted element in the batch to get the class (it is the maximum index of the num_classes array per batch sample) (look at torch.argmax in the PyTorch documentation)\n",
        "  # (2) compare the predicted class index with the index in its corresponding neighbor within label_batch \n",
        "  # (3) sum up the number of affirmative comparisons and return the summation\n",
        "def correct_predictions(predicted_batch, label_batch):\n",
        "  pred = predicted_batch.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "  acum = pred.eq(label_batch.view_as(pred)).sum().item()\n",
        "  return acum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcKupxKKBVR2"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Next, the training and test functions that iterate for an epoch are defined. They are missing a key ingredient to make the `data` tensors, which include images, compatible with the fully connected input of size `num_inputs` of the network. Please fix this in both cases, remembering the use of `view()` in the previous lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaOi6FK6sLwj"
      },
      "source": [
        "def train_epoch(train_loader, network, optimizer, criterion, hparams):\n",
        "\n",
        "  # Activate the train=True flag inside the model\n",
        "  network.train()\n",
        "  \n",
        "  device = hparams['device']\n",
        "  avg_loss = None\n",
        "  avg_weight = 0.1\n",
        "\n",
        "  # For each batch\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      # TO DO\n",
        "      data = TO DO\n",
        "      \"\"\" SOLUCIO: data = data.view(data.shape[0], -1) \"\"\"\n",
        "      output = network(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      if avg_loss:\n",
        "        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\n",
        "      else:\n",
        "        avg_loss = loss.item()\n",
        "      optimizer.step()\n",
        "      if batch_idx % hparams['log_interval'] == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsZCX8ajspqf"
      },
      "source": [
        "def test_epoch(test_loader, network, hparams):\n",
        "\n",
        "    # Dectivate the train=True flag inside the model\n",
        "    network.eval()\n",
        "    \n",
        "    device = hparams['device']\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "\n",
        "            # Load data and feed it through the neural network\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            data = data.view(data.shape[0], -1)\n",
        "            output = network(data)\n",
        "\n",
        "            test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\n",
        "            # WARNING: If you are using older Torch versions, the previous call may need to be replaced by\n",
        "            # test_loss += criterion(output, target, size_average=False).item()\n",
        "\n",
        "            # compute number of correct predictions in the batch\n",
        "            acc += correct_predictions(output, target)\n",
        "\n",
        "    # Average accuracy across all correct predictions batches now\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = 100. * acc / len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, acc, len(test_loader.dataset), test_acc,\n",
        "        ))\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4IzI8Ki7Mko"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Now that the `network` and the training and test functions are good to go epoch by epoch, build the loop to make as many as `hparams['num_epochs']` epochs by alternating the train and test phases. Check what these functions return and store the resulting values in lists that will be used to plot the results in the later cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvlI9HzB7vVx"
      },
      "source": [
        "# Init lists to save the evolution of the training & test losses/accuracy.\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accs = []\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(1, hparams['num_epochs'] + 1):\n",
        "\n",
        "  # Compute & save the average training loss for the current epoch\n",
        "  train_loss = train_epoch(train_loader, network, optimizer, criterion, hparams)\n",
        "  train_losses.append(train_loss)\n",
        "\n",
        "  # TODO: Compute & save the average test loss & accuracy for the current epoch\n",
        "  # TIP: Review the functions previously defined to implement the train/test epochs \n",
        "  test_loss,test_accuracy=TODO  \n",
        "  test_losses.append(test_loss)\n",
        "  test_accs.append(test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMyCUkl8Jfb9"
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(2,1,1)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('NLLLoss')\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(test_losses, label='test')\n",
        "plt.legend()\n",
        "plt.subplot(2,1,2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy [%]')\n",
        "plt.plot(test_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkAhNAqvNCBs"
      },
      "source": [
        "#### References\n",
        "\n",
        "[1] https://github.com/pytorch/examples/blob/master/mnist/main.py"
      ]
    }
  ]
}