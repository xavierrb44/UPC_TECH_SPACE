{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_cnn_live.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCJc0mIHsTeH"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agBrnjOaHPMa"
      },
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkAnCSI6HUvj"
      },
      "source": [
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(\"data\", download=True, transform=transform, train=True)\n",
        "test_dataset = datasets.MNIST(\"data\", download=True, transform=transform, train=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWXy_b2sHpXV"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O51fejt2IF9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d93f82ae-a202-457b-d914-3cf8cd7eac39"
      },
      "source": [
        "image, label = train_dataset[10]\n",
        "print(image.shape)\n",
        "print(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH3rHZn_IMme",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e170c409-ab79-4cfd-f2f4-3de23c3f9326"
      },
      "source": [
        "linear_layer = nn.Linear(100, 200)\n",
        "x = torch.rand(64, 100)\n",
        "y = linear_layer(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3ubB6BcI5Q6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "438065dd-6d37-43ac-a0b1-a0f398bfe04a"
      },
      "source": [
        "conv_layer = nn.Conv2d(3, 10, kernel_size=3)\n",
        "x = torch.rand(64, 3, 150, 150)\n",
        "y = conv_layer(x)\n",
        "print(y.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10, 148, 148])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlZ_AE1oL7lZ"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Re-define the `conv` layer below setting the appropriate property such that the output spatial shape is the same as the input one. Look at the PyTorch documentation (`https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d`) for more reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyuJkd3OKCQm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "16ee1968-7053-43b0-ef0c-11a6bc10cd3b"
      },
      "source": [
        "# TO DO: define the conv layer below and ensure that the output tensor shape in dimensions {H, W} \n",
        "#( as in [1, channels, H, W] ) will be the same as the input in both cases.\n",
        "conv = torch.nn.Conv2d(1, 10, 3, padding=1)\n",
        "\n",
        "x = torch.rand(1, 1, 20, 20)\n",
        "y = conv(x)\n",
        "print(y.shape)\n",
        "\n",
        "x = torch.rand(1, 1, 11, 11)\n",
        "y = conv(x)\n",
        "print(y.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10, 20, 20])\n",
            "torch.Size([1, 10, 11, 11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjtUwF8CNyAL"
      },
      "source": [
        "## Pooling\n",
        "\n",
        "![](https://qph.fs.quoracdn.net/main-qimg-40cdeb3b43594f4b1b1b6e2c137e80b7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8GN5o-5Naav"
      },
      "source": [
        "NUM_BITS_FLOAT32 = 32\n",
        "\n",
        "class CNNMemAnalyzer(nn.Module):\n",
        "\n",
        "  def __init__(self, layers):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "  \n",
        "  def forward(self, x):\n",
        "    tot_mbytes = 0\n",
        "    spat_res = []\n",
        "    for layer in self.layers:\n",
        "      h = layer(x)\n",
        "      mem_h_bytes = np.cumprod(h.shape)[-1] * NUM_BITS_FLOAT32 // 8\n",
        "      mem_h_mb = mem_h_bytes / 1e6\n",
        "      print('-' * 30)\n",
        "      print('New feature map of shape: ', h.shape)\n",
        "      print('Mem usage: {} MB'.format(mem_h_mb))\n",
        "      x = h\n",
        "      if isinstance(layer, nn.Conv2d):\n",
        "        # keep track of the current spatial width for conv layers\n",
        "        spat_res.append(h.shape[-1])\n",
        "      tot_mbytes += mem_h_mb\n",
        "    print('=' * 30)\n",
        "    print('Total used memory: {:.2f} MB'.format(tot_mbytes))\n",
        "    return tot_mbytes, spat_res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9GHHQTWOoyb"
      },
      "source": [
        "cnn = CNNMemAnalyzer(nn.ModuleList([\n",
        "                                    nn.Conv2d(1, 32, 3),\n",
        "                                    nn.Conv2d(32, 64, 3),\n",
        "                                    nn.Conv2d(64, 64, 3),\n",
        "                                    nn.Conv2d(64, 128, 3),\n",
        "                                    nn.Conv2d(128, 512, 3),\n",
        "]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIjJ1EXcPNQJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "ca7067f1-f967-4e09-8545-2d62d6f931ba"
      },
      "source": [
        "tot_mbytes, spat_res = cnn(torch.randn(1, 1, 512, 512))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 32, 510, 510])\n",
            "Mem usage: 33.2928 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 64, 508, 508])\n",
            "Mem usage: 66.064384 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 64, 506, 506])\n",
            "Mem usage: 65.545216 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 128, 504, 504])\n",
            "Mem usage: 130.056192 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 512, 502, 502])\n",
            "Mem usage: 516.104192 MB\n",
            "==============================\n",
            "Total used memory: 811.06 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q7-UDSOPXgc"
      },
      "source": [
        "cnn = CNNMemAnalyzer(nn.ModuleList([\n",
        "                                    nn.Conv2d(1, 32, 3),\n",
        "                                    nn.MaxPool2d(2),\n",
        "                                    nn.Conv2d(32, 64, 3),\n",
        "                                    nn.MaxPool2d(2),\n",
        "                                    nn.Conv2d(64, 64, 3),\n",
        "                                    nn.MaxPool2d(2),\n",
        "                                    nn.Conv2d(64, 128, 3),\n",
        "                                    nn.MaxPool2d(2),\n",
        "                                    nn.Conv2d(128, 512, 3),\n",
        "]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YntdbJyIP5Oe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "a239b8dc-13e0-4063-b0b8-b8c8dbd9dee6"
      },
      "source": [
        "tot_mbytes, spat_res = cnn(torch.randn(1, 1, 512, 512))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 32, 510, 510])\n",
            "Mem usage: 33.2928 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 32, 255, 255])\n",
            "Mem usage: 8.3232 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 64, 253, 253])\n",
            "Mem usage: 16.386304 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 64, 126, 126])\n",
            "Mem usage: 4.064256 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 64, 124, 124])\n",
            "Mem usage: 3.936256 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 64, 62, 62])\n",
            "Mem usage: 0.984064 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 128, 60, 60])\n",
            "Mem usage: 1.8432 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 128, 30, 30])\n",
            "Mem usage: 0.4608 MB\n",
            "------------------------------\n",
            "New feature map of shape:  torch.Size([1, 512, 28, 28])\n",
            "Mem usage: 1.605632 MB\n",
            "==============================\n",
            "Total used memory: 70.90 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky5MoOCqRe4o"
      },
      "source": [
        "# How LeNet works\n",
        "\n",
        "![](https://miro.medium.com/max/2154/1*1TI1aGBZ4dybR6__DI9dzA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtk7Dm7oRvKY"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Make the `ConvBlock` class to properly do: `Conv2d`, `ReLU`, and `MaxPool2d`. Ensure that for an input of size `1x32x32` you obtain an output feature map of size `6x14x14` as shown in the figure above for layer `S2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj817xGgP6R7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42e258e3-b3f4-4950-cc15-92280c105ef7"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, num_inp_channels, num_out_fmaps, \n",
        "               kernel_size, pool_size=2):\n",
        "    super().__init__()\n",
        "    # TODO: define the 3 modules needed\n",
        "    self.conv = nn.Conv2d(num_inp_channels, num_out_fmaps, kernel_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxpool = nn.MaxPool2d(pool_size)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.maxpool(self.relu(self.conv(x)))\n",
        "\n",
        "\n",
        "block = ConvBlock(1, 10, 3)\n",
        "x = torch.rand(64, 1, 28, 28)\n",
        "y = block(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10, 13, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDUPQOE1R-wI"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Finish the `PseudoLeNet` class by including the following: \n",
        "1. As the input images from MNIST are 28x28, add padding to make them 32x32 with the `torch.nn.ConstantPad2d` (https://pytorch.org/docs/stable/nn.html#torch.nn.ConstantPad2d).\n",
        "2. Build the `mlp` classifier as a `nn.Sequential` stack of fully connected layers and ReLU activations, with the sizes shown in the figure above: [120, 84, 10]. Plug the appropriate output activation in the end to do multi-class classification.\n",
        "3. Remember to \"flatten\" the feature maps coming out of the second `ConvBlock` and connect them to the output `mlp` to build the classifier in the `forward` function. This has to be done because fully connected layers (`Linear`) only accept features without any spatial dimension. Hence, all these spatial dimensions and channels are unrolled into single vectors, one per batch sample. **HINT: Remember the `.view()` operator to change tensors shape!**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyGCG4HiR_Xn"
      },
      "source": [
        "class PseudoLeNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # TODO: Define the padding\n",
        "    self.pad = nn.ConstantPad2d(2, 0)\n",
        "    self.conv1 = ConvBlock(1, 6, 5)\n",
        "    self.conv2 = ConvBlock(6, 16, 5)\n",
        "    # TODO: Define the MLP at the deepest layers\n",
        "    self.mlp = nn.Sequential(\n",
        "        # nn.Flatten(start_dim=1),\n",
        "        nn.Linear(5*16*16, 120),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(120, 84),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(84, 10),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x  [1, 28, 28] \n",
        "    x = self.pad(x)  # [1, 32, 32] \n",
        "    x = self.conv1(x)  # [6, 14, 14] \n",
        "    x = self.conv2(x)  # [16, 5, 5] \n",
        "    # Obtain the parameters of the tensor in terms of:\n",
        "    # 1) batch size\n",
        "    # 2) number of channels\n",
        "    # 3) spatial \"height\"\n",
        "    # 4) spatial \"width\"\n",
        "    bsz, nch, height, width = x.shape\n",
        "    # TODO: Flatten the feature map with the view() operator \n",
        "    # within each batch sample  \n",
        "    x = x.view(bsz, -1)\n",
        "    y = self.mlp(x)\n",
        "    return y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ4udfK5XlF7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}