{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aidl_2020s_dl_lab11_gan_todo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbRVQ73oRVAO"
      },
      "source": [
        "# DCGAN + conditional DCGAN\n",
        "\n",
        "\n",
        "In this notebook will learn about Generative Adversarial Networks by implementing a DCGAN to generate images from noise, followed by a conditional DCGAN.\n",
        "\n",
        "**Important:** Set the Colab environment to run on GPU\n",
        "\n",
        "**Notebook created by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2020).**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcZGU22N9IX2"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, datasets, utils\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import math\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_wpixTJ9fde"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCW_8O999hAm"
      },
      "source": [
        "num_epochs = 300\n",
        "\n",
        "lr = 0.0002\n",
        "betas = (0.5, 0.999)\n",
        "\n",
        "noise_size = 100\n",
        "batch_size = 128\n",
        "num_val_samples = 25\n",
        "num_classes = 10\n",
        "num_input_channels = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1AaRIWR9L5m"
      },
      "source": [
        "## Dataset\n",
        "Download and prepare dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPMYeOfU9SyD"
      },
      "source": [
        "train_transforms = transforms.Compose(\n",
        "            [   \n",
        "                transforms.Resize(32),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,), (.5,))\n",
        "            ])\n",
        "dataset = datasets.MNIST(root='data', train=True, transform=train_transforms, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDFqjh4T9a7W"
      },
      "source": [
        "## Data Loader\n",
        "Create a data loader for the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKNGNOBQ9U4w"
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--drGQMaGeop"
      },
      "source": [
        "# DCGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFEBhsZb96T7"
      },
      "source": [
        "## Networks\n",
        "First, lets define our simple generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD8ms_VzNmWn"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "The generator takes random noise as input and gives an image as output. Your exercise is to create the generator model.\n",
        "\n",
        "It should follow these guidelines:\n",
        "* The input will be a vector with random noise of size `noise_size`\n",
        "* You should first apply a fully connected with output size 512\\*4\\*4 (channels\\*height\\*width)\n",
        "* Then you should apply 3 blocks of:\n",
        "    * TransposedConvolution with kernel size 4, stride 2 and padding 1. For the first 2 blocks, the output channels should be 256 and 128. For the third block, the output channels should be the correct one to generate images of the dataset.\n",
        "    * BatchNorm2d except for the last block.\n",
        "    * ReLU activation for the first 2 blocks and Tanh for the third block.\n",
        "\n",
        "**Hint**: Remember to use reshape where necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA2vnc-G9-Nl"
      },
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "      \n",
        "        self.fc = ...\n",
        "\n",
        "        self.convt1 = nn.Sequential(\n",
        "            ...\n",
        "            ...\n",
        "            ...\n",
        "        )\n",
        "        self.convt2 = nn.Sequential(\n",
        "            ...\n",
        "            ...\n",
        "            ...\n",
        "        )\n",
        "        self.convt3 = nn.Sequential(\n",
        "            ...\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ...\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnSOu07Q-FLh"
      },
      "source": [
        "Similarly lets define a simple discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BQFM-vIPHvj"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "The discriminator takes an image as input and classifies it between Real or Fake (1 or 0). Your exercise is to create the discriminator model.\n",
        "\n",
        "It should follow these guidelines:\n",
        "* The input will be an image of size `[num_input_channels, 32, 32]`\n",
        "* You should apply 3 blocks of:\n",
        "    * Convolution with kernel size 4, stride 2 and padding 1. The output channels should be 128, 256 and 512.\n",
        "    * BatchNorm2d except for the first block.\n",
        "    * LeakyReLU activation (alpha=0.2)\n",
        "* Then you should first apply a fully connected with input size 512\\*4\\*4 (channels\\*height\\*width) and the correct output size and activation for binary classification\n",
        "\n",
        "\n",
        "**Hint**: Remember to use reshape/flatten where necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI6fdck8-Q2N"
      },
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            ...\n",
        "            ...\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            ...\n",
        "            ...\n",
        "            ...\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            ...\n",
        "            ...\n",
        "            ...\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            ...\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ...\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpmnuKnAeKjY"
      },
      "source": [
        "generator = Generator().to(device)\n",
        "optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "discriminator = Discriminator().to(device)\n",
        "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) in {nn.Conv2d, nn.ConvTranspose2d, nn.Linear}:\n",
        "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if m.bias != None:\n",
        "            torch.nn.init.constant_(m.bias, 0.0)\n",
        "    if type(m) == nn.BatchNorm2d:\n",
        "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "generator.apply(init_weights)\n",
        "discriminator.apply(init_weights);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNKQKA4FfCL7"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryDTioLaSd_t"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Complete the code. Take into account which labels should be used at each step of the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp8UARzzfECJ"
      },
      "source": [
        "def train_batch(real_samples, generator, discriminator, optimizer_g, optimizer_d):\n",
        "\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    \n",
        "    current_batch_size = real_samples.shape[0]\n",
        "    label_real = torch.ones(current_batch_size, 1, device=device)\n",
        "    label_fake = torch.zeros(current_batch_size, 1, device=device)\n",
        "\n",
        "    ####################\n",
        "    # OPTIMIZE GENERATOR\n",
        "    ####################\n",
        "\n",
        "    # Reset gradients\n",
        "    optimizer_g.zero_grad()\n",
        "\n",
        "    # Generate fake samples\n",
        "    z = torch.randn(current_batch_size, noise_size, device=device)\n",
        "    fake_samples = generator(z)\n",
        "    \n",
        "    # Evaluate the generated samples with the discriminator\n",
        "    predictions_g_fake = discriminator(fake_samples)\n",
        "\n",
        "    # Calculate error with respect to what the generator wants\n",
        "    loss_g = ...\n",
        "\n",
        "    # Backpropagate\n",
        "    loss_g.backward()\n",
        "    \n",
        "    # Update weights\n",
        "    optimizer_g.step()\n",
        "    \n",
        "    ########################\n",
        "    # OPTIMIZE DISCRIMINATOR\n",
        "    ########################\n",
        "    \n",
        "    fake_samples = fake_samples.detach()\n",
        "    \n",
        "    # Reset gradients\n",
        "    optimizer_d.zero_grad()\n",
        "\n",
        "    # Calculate discriminator prediction for real samples\n",
        "    predictions_d_real = discriminator(real_samples)  \n",
        "\n",
        "    # Calculate error with respect to what the discriminator wants\n",
        "    loss_d_real = ...\n",
        "\n",
        "    # Calculate discriminator loss for fake samples\n",
        "    predictions_d_fake = discriminator(fake_samples)\n",
        "\n",
        "    # Calculate error with respect to what the discriminator wants\n",
        "    loss_d_fake = ...\n",
        "    \n",
        "    # Total discriminator loss\n",
        "    loss_d = (loss_d_real + loss_d_fake) / 2\n",
        "    loss_d.backward()\n",
        "\n",
        "    optimizer_d.step()\n",
        "\n",
        "    return loss_g.item(), loss_d.item()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbBC3H3j0Yal"
      },
      "source": [
        "## Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAokfOJ8iBpQ"
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(generator, z_val):\n",
        "    generator.eval()\n",
        "    fake_samples = generator(z_val).cpu()\n",
        "    # select a sample or create grid if img is a batch\n",
        "    nrows = int(math.sqrt(fake_samples.shape[0]))\n",
        "    img = utils.make_grid(fake_samples, nrow=nrows)\n",
        "\n",
        "    # unnormalize\n",
        "    img = (img*0.5 + 0.5)*255\n",
        "\n",
        "    # to numpy\n",
        "    image_numpy = img.numpy().astype(np.uint8)\n",
        "    image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
        "    return Image.fromarray(image_numpy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWS0uvRJEHGd"
      },
      "source": [
        "## Train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd3efLyTEQSE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "35e20305-b8f2-4f44-bac0-4b1363085da3"
      },
      "source": [
        "z_val = torch.randn(num_val_samples, noise_size, device=device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for i, (real_samples, labels) in enumerate(dataloader):\n",
        "        real_samples = real_samples.to(device)\n",
        "        loss_g, loss_d = train_batch(real_samples, generator, discriminator, optimizer_g, optimizer_d)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            fake_images = evaluate(generator, z_val)\n",
        "            display(fake_images)\n",
        "\n",
        "            # Show current loss\n",
        "            print(f\"epoch: {epoch+1}/{num_epochs} batch: {i+1}/{len(dataloader)} G_loss: {loss_g}, D_loss: {loss_d}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKwAAACsCAIAAABpU9CPAAABYElEQVR4nO3SMQEAIAzAsILxWUfGDhIFPXpmJv52twPYZwJMgAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATUD3eHwLVOCIj9gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=172x172 at 0x7FE2E1599B38>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1/300 batch: 1/469 G_loss: 0.6909956336021423, D_loss: 0.6923298239707947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-5c899f63982b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mreal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-99-85d729ff5aaa>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(real_samples, generator, discriminator, optimizer_g, optimizer_d)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0moptimizer_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FgkBXj-9Jgq"
      },
      "source": [
        "# Extra: Conditional GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdXFJ_B3E0l1"
      },
      "source": [
        "## Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbf_CtNvS2uT"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "We will now modify the generator from before to a conditional generator. To do it, we will concatenated the input to the convolutions with an embedding of the label we want to generate.\n",
        "\n",
        "Complete the forward method. To do it, use the embedding layer with the label, and then use `torch.cat` to concatenate the label as a channel (after the corresponding `reshape`)\n",
        "\n",
        "**Hint**: The embedding is concatenated as a new channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSsyAao5f0Dw"
      },
      "source": [
        "class ConditionalGenerator(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc = torch.nn.Linear(noise_size, 512*4*4)\n",
        "\n",
        "        self.embedding = nn.Embedding(num_classes, 4*4)\n",
        "\n",
        "        self.convt1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=512+1, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.convt2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.convt3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=num_input_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            torch.nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        x = self.fc(x)\n",
        "        x = x.reshape(x.shape[0], 512, 4, 4)\n",
        "\n",
        "        ...\n",
        "\n",
        "        x = self.convt1(x)\n",
        "        x = self.convt2(x)\n",
        "        x = self.convt3(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhtPC283Tbkv"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "We will now modify the discriminator from before to a conditional discriminator. To do it, we will concatenated the input image with an embedding of the label we want to generate.\n",
        "\n",
        "Complete the forward method. To do it, use the embedding layer with the label, and then use `torch.cat` to concatenate the label as a channel (after the corresponding `reshape`)\n",
        "\n",
        "**Hint**: The embedding is concatenated as a new channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFiJKxTL9Q3n"
      },
      "source": [
        "class ConditionalDiscriminator(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_classes, 32*32)\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=num_input_channels+1, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512*4*4, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, label):\n",
        "\n",
        "\n",
        "        ...\n",
        "\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj4qU61KBWj1"
      },
      "source": [
        "generator = ConditionalGenerator().to(device)\n",
        "optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "discriminator = ConditionalDiscriminator().to(device)\n",
        "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) in {nn.Conv2d, nn.ConvTranspose2d, nn.Linear}:\n",
        "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if m.bias != None:\n",
        "            torch.nn.init.constant_(m.bias, 0.0)\n",
        "    if type(m) == nn.BatchNorm2d:\n",
        "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "generator.apply(init_weights)\n",
        "discriminator.apply(init_weights);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85_bncGME4VC"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH_e4ra5-7YP"
      },
      "source": [
        "def train_batch_conditional(real_samples, real_labels, generator, discriminator, optimizer_g, optimizer_d):\n",
        "\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    current_batch_size = real_samples.shape[0]\n",
        "    label_real = torch.ones(current_batch_size, 1, device=device)\n",
        "    label_fake = torch.zeros(current_batch_size, 1, device=device)\n",
        "\n",
        "    ####################\n",
        "    # OPTIMIZE GENERATOR\n",
        "    ####################\n",
        "\n",
        "    # Reset gradients\n",
        "    optimizer_g.zero_grad()\n",
        "\n",
        "    # Generate fake samples\n",
        "    z = torch.randn(current_batch_size, noise_size, device=device)\n",
        "    fake_labels = torch.randint(0, 10, size=(current_batch_size, 1), device=device)\n",
        "    fake_samples = generator(z, fake_labels)\n",
        "    \n",
        "    # Evaluate the generated samples with the discriminator\n",
        "    predictions_g_fake = discriminator(fake_samples, fake_labels)\n",
        "\n",
        "    # Calculate error with respect to what the generator wants\n",
        "    loss_g = criterion(predictions_g_fake, label_real)\n",
        "\n",
        "    # Backpropagate\n",
        "    loss_g.backward()\n",
        "    \n",
        "    # Update weights\n",
        "    optimizer_g.step()\n",
        "\n",
        "    ####################\n",
        "    # OPTIMIZE DISCRIMINATOR\n",
        "    ####################\n",
        "\n",
        "    fake_samples = fake_samples.detach()\n",
        "    # Reset gradients\n",
        "    optimizer_d.zero_grad()\n",
        "\n",
        "    # Calculate discriminator prediction for real samples\n",
        "    predictions_d_real = discriminator(real_samples, real_labels)  \n",
        "\n",
        "    # Calculate error with respect to what the discriminator wants\n",
        "    loss_d_real = criterion(predictions_d_real, label_real)\n",
        "\n",
        "    # Calculate discriminator loss for fake samples\n",
        "    predictions_d_fake = discriminator(fake_samples, fake_labels)\n",
        "\n",
        "    # Calculate error with respect to what the discriminator wants\n",
        "    loss_d_fake = criterion(predictions_d_fake, label_fake)\n",
        "    \n",
        "    # Total discriminator loss\n",
        "    loss_d = (loss_d_real + loss_d_fake) / 2\n",
        "    loss_d.backward()\n",
        "\n",
        "    optimizer_d.step()\n",
        "\n",
        "    return loss_g.item(), loss_d.item()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cZiJBr4E6P_"
      },
      "source": [
        "## Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkLdQfIsBC3e"
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_conditional(generator, z_val, labels_val):\n",
        "    generator.eval()\n",
        "    fake_samples = generator(z_val, labels_val).cpu()\n",
        "    # select a sample or create grid if img is a batch\n",
        "    nrows = int(math.sqrt(fake_samples.shape[0]))\n",
        "    img = utils.make_grid(fake_samples, nrow=nrows)\n",
        "\n",
        "    # unnormalize\n",
        "    img = (img*0.5 + 0.5)*255\n",
        "\n",
        "    # to numpy\n",
        "    image_numpy = img.numpy().astype(np.uint8)\n",
        "    image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
        "    return Image.fromarray(image_numpy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-g63Js1E9N7"
      },
      "source": [
        "## Train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edqWbdfp_4ra",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "aab2976e-11e6-49e9-9854-78afa1f7e003"
      },
      "source": [
        "from itertools import cycle\n",
        "\n",
        "z_val = torch.randn(num_val_samples, noise_size, device=device)\n",
        "labels_cycle = cycle(range(num_classes))\n",
        "labels_val = torch.tensor([next(labels_cycle) for i in range(num_val_samples)], device=device).unsqueeze(1)\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for i, (real_samples, real_labels) in enumerate(dataloader):\n",
        "        real_samples = real_samples.to(device)\n",
        "        real_labels = real_labels.unsqueeze(1).to(device)\n",
        "        loss_g, loss_d = train_batch_conditional(real_samples, real_labels, generator, discriminator, optimizer_g, optimizer_d)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            fake_images = evaluate_conditional(generator, z_val, labels_val)\n",
        "            display(fake_images)\n",
        "\n",
        "            # Show current loss\n",
        "            print(f\"epoch: {epoch+1}/{num_epochs} batch: {i+1}/{len(dataloader)} G_loss: {loss_g}, D_loss: {loss_d}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKwAAACsCAIAAABpU9CPAAABYElEQVR4nO3SMQEAIAzAsILxWUfGDhIFPXpmJv52twPYZwJMgAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATkAnIBGQCMgGZgExAJiATUD3eHwLVOCIj9gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=172x172 at 0x7FE2E0056358>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1/300 batch: 1/469 G_loss: 0.6925517320632935, D_loss: 0.6937770843505859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x5eZTInG_u4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}