{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioClassification with solutions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROyb8uauZFb3"
      },
      "source": [
        "# Audio Classification with Mel-Spectrogram filterbanks\n",
        "\n",
        "For this tutorial we will use the audio samples from Google Speech Commands Dataset v0.01 https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data\n",
        "\n",
        "The audio samples consist of recordings of simple commands of duration of 1 second. Twenty core command words were recorded, with most speakers saying each of them five times. \n",
        "The core words are\n",
        "\"Yes\", \"No\", \"Up\", \"Down\", \"Left\", \"Right\", \"On\", \"Off\", \"Stop\", \"Go\", \"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", and \"Nine\". \n",
        "\n",
        "To help distinguish unrecognized words, there are also ten auxiliary words, which most speakers only said once. These include \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\", \"Tree\", and \"Wow\".\n",
        "\n",
        "There is also long samples of background noise / silence.\n",
        "\n",
        "For convenience, three list of files are provided for train/validation/test splits.\n",
        "\n",
        "First let's download and extract the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH0Q9Xm12Jgg"
      },
      "source": [
        "import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms, datasets\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import pickle\n",
        "import hashlib\n",
        "import librosa\n",
        "from scipy.io import wavfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Cu_keatsgk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "6f7813d4-d884-44ba-87a3-06a9ccc236d7"
      },
      "source": [
        "!pip install torchsummary\n",
        "#example https://github.com/GRAAL-Research/poutyne/blob/master/examples/mnist.ipynb\n",
        "!pip install poutyne"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Requirement already satisfied: poutyne in /usr/local/lib/python3.6/dist-packages (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from poutyne) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from poutyne) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->poutyne) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz65D-Ef-MON",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "efed2b41-0d2b-4d37-abfd-af102aeaca24"
      },
      "source": [
        "!pip install gdown\n",
        "!gdown \"https://drive.google.com/uc?id=1HR28jRFwrveq5zxjkzri61jm9F4-M7p7\"\n",
        "!tar -zxf speech_commands_v0.01_with_splits.tar.gz\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HR28jRFwrveq5zxjkzri61jm9F4-M7p7\n",
            "To: /content/speech_commands_v0.01_with_splits.tar.gz\n",
            "1.49GB [00:10, 138MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpQmJqxMbwCw"
      },
      "source": [
        "# Define Class for computing Mel-Spectrogram\n",
        "\n",
        "As seen in previous colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTiLABiib7cd"
      },
      "source": [
        "#Helper Functions borrowed from torchaudio https://github.com/pytorch/audio/blob/master/torchaudio/transforms.py\n",
        "\n",
        "class PadTrim(object):\n",
        "    \"\"\"Pad/Trim a 1d-Tensor (Signal or Labels)\n",
        "    Args:\n",
        "        tensor (Tensor): Tensor of audio of size (n x c) or (c x n)\n",
        "        max_len (int): Length to which the tensor will be padded\n",
        "        channels_first (bool): Pad for channels first tensors.  Default: `True`\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_len, fill_value=0, channels_first=True):\n",
        "        self.max_len = max_len\n",
        "        self.fill_value = fill_value\n",
        "        self.len_dim, self.ch_dim = int(channels_first), int(not channels_first)\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            Tensor: (c x n) or (n x c)\n",
        "        \"\"\"\n",
        "        assert tensor.size(self.ch_dim) < 128, \\\n",
        "            \"Too many channels ({}) detected, see channels_first param.\".format(tensor.size(self.ch_dim))\n",
        "        if self.max_len > tensor.size(self.len_dim):\n",
        "            padding = [self.max_len - tensor.size(self.len_dim)\n",
        "                       if (i % 2 == 1) and (i // 2 != self.len_dim)\n",
        "                       else 0\n",
        "                       for i in range(4)]\n",
        "            with torch.no_grad():\n",
        "                tensor = torch.nn.functional.pad(tensor, padding, \"constant\", self.fill_value)\n",
        "        elif self.max_len < tensor.size(self.len_dim):\n",
        "            tensor = tensor.narrow(self.len_dim, 0, self.max_len)\n",
        "        return tensor\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(max_len={0})'.format(self.max_len)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MelScale(object):\n",
        "    \"\"\"This turns a normal STFT into a mel frequency STFT, using a conversion\n",
        "       matrix.  This uses triangular filter banks.\n",
        "    Args:\n",
        "        n_mels (int): number of mel bins\n",
        "        sr (int): sample rate of audio signal\n",
        "        f_max (float, optional): maximum frequency. default: `sr` // 2\n",
        "        f_min (float): minimum frequency. default: 0\n",
        "        n_stft (int, optional): number of filter banks from stft. Calculated from first input\n",
        "            if `None` is given.  See `n_fft` in `Spectrogram`.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_mels=128, sr=16000, f_max=None, f_min=0., n_stft=None):\n",
        "        self.n_mels = n_mels\n",
        "        self.sr = sr\n",
        "        self.f_max = f_max if f_max is not None else sr // 2\n",
        "        self.f_min = f_min\n",
        "        self.fb = self._create_fb_matrix(n_stft) if n_stft is not None else n_stft\n",
        "\n",
        "    def __call__(self, spec_f):\n",
        "        if self.fb is None:\n",
        "            self.fb = self._create_fb_matrix(spec_f.size(2)).to(spec_f.device)\n",
        "        else:\n",
        "            # need to ensure same device for dot product\n",
        "            self.fb = self.fb.to(spec_f.device)\n",
        "        spec_m = torch.matmul(spec_f, self.fb)  # (c, l, n_fft) dot (n_fft, n_mels) -> (c, l, n_mels)\n",
        "        return spec_m\n",
        "\n",
        "    def _create_fb_matrix(self, n_stft):\n",
        "        \"\"\" Create a frequency bin conversion matrix.\n",
        "        Args:\n",
        "            n_stft (int): number of filter banks from spectrogram\n",
        "        \"\"\"\n",
        "\n",
        "        # get stft freq bins\n",
        "        stft_freqs = torch.linspace(self.f_min, self.f_max, n_stft)\n",
        "        # calculate mel freq bins\n",
        "        m_min = 0. if self.f_min == 0 else self._hertz_to_mel(self.f_min)\n",
        "        m_max = self._hertz_to_mel(self.f_max)\n",
        "        m_pts = torch.linspace(m_min, m_max, self.n_mels + 2)\n",
        "        f_pts = self._mel_to_hertz(m_pts)\n",
        "        # calculate the difference between each mel point and each stft freq point in hertz\n",
        "        f_diff = f_pts[1:] - f_pts[:-1]  # (n_mels + 1)\n",
        "        slopes = f_pts.unsqueeze(0) - stft_freqs.unsqueeze(1)  # (n_stft, n_mels + 2)\n",
        "        # create overlapping triangles\n",
        "        z = torch.tensor(0.)\n",
        "        down_slopes = (-1. * slopes[:, :-2]) / f_diff[:-1]  # (n_stft, n_mels)\n",
        "        up_slopes = slopes[:, 2:] / f_diff[1:]  # (n_stft, n_mels)\n",
        "        fb = torch.max(z, torch.min(down_slopes, up_slopes))\n",
        "        return fb\n",
        "\n",
        "    def _hertz_to_mel(self, f):\n",
        "        return 2595. * torch.log10(torch.tensor(1.) + (f / 700.))\n",
        "\n",
        "    def _mel_to_hertz(self, mel):\n",
        "        return 700. * (10**(mel / 2595.) - 1.)\n",
        "      \n",
        "class MelSpectrogram(nn.Module):\n",
        "    def __init__(self, n_mels = 40, sfr=16000):\n",
        "        super(MelSpectrogram, self).__init__()\n",
        "        self.sfr = sfr\n",
        "        self.window_stride=0.01\n",
        "        self.window_size=0.02\n",
        "        self.n_fft=512\n",
        "        self.n_mels=n_mels\n",
        "        \n",
        "        self.win_length = int(self.sfr * self.window_size)\n",
        "        self.hop_length = int(self.sfr * self.window_stride)\n",
        "        self.lowfreq = 20\n",
        "        self.highfreq = self.sfr/2 - 400\n",
        "        self.window = torch.hamming_window(self.win_length)\n",
        "        \n",
        "        self.mel = MelScale(n_mels=self.n_mels, sr=self.sfr, f_max=self.highfreq, f_min=self.lowfreq)\n",
        "        self.norm = nn.InstanceNorm2d(1)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \n",
        "        x = x.squeeze(1)\n",
        "        spec_f = torch.stft(x, n_fft=self.n_fft, hop_length=self.hop_length, \n",
        "                    win_length=self.win_length, \n",
        "                    window=self.window,\n",
        "                    center=True,\n",
        "                    normalized=False, onesided=True,\n",
        "                    pad_mode='reflect'\n",
        "                   )\n",
        "        spec_f = spec_f.pow(2).sum(-1)\n",
        "        x = self.mel(spec_f.transpose(1,2)).transpose(1,2)\n",
        "        x = torch.log(x+0.0001)\n",
        "        x = x.unsqueeze(1)\n",
        "        #x = self.norm(x)\n",
        "        return x\n",
        "      \n",
        "      \n",
        "    def plot_sample(self, fbank, index):\n",
        "        librosa.display.specshow(fbank[index,:,:,:].view(self.n_mels,-1).numpy(),\n",
        "                          y_axis='mel', x_axis='time',sr=self.sfr, fmax=self.highfreq, hop_length=self.hop_length)\n",
        "        plt.title('Mel spectrogram')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y-K1ePceHot"
      },
      "source": [
        "#AudioReader for wavefiles\n",
        "\n",
        "This reader provides data in the format X, Y = (raw_audio, target_class)\n",
        "\n",
        "**add_silence_class** parameter: includes random samples from background noise to the dataset in the sample proportion as the \"yes\" class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdwsRHAH4VJj"
      },
      "source": [
        "class AudioReader(data.Dataset):\n",
        "  \n",
        "    def __init__(self, list_path, transform=PadTrim(16000), add_silence_class=False, add_noise=False):\n",
        "        \n",
        "        self.list_path = list_path\n",
        "        self.database_path = os.path.dirname(list_path) + '/audio/'\n",
        "        self.add_noise = add_noise\n",
        "        self.add_silence_class = add_silence_class\n",
        "        self.transform = transform\n",
        "\n",
        "        self.target_class = {}\n",
        "        self.target_class_idx_to_name = {}\n",
        "        self.target_class_names = ['unknown','silence', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
        "        for i, name in enumerate(self.target_class_names):\n",
        "            self.target_class[name] = i\n",
        "            self.target_class_idx_to_name[i] = name\n",
        "        self.audio_class = {}\n",
        "        self.audio_speaker = {}\n",
        "        self.audios = []\n",
        "                 \n",
        "        self.read_list_data()\n",
        "                 \n",
        "        self.speaker_ids = {}\n",
        "        for i, spk_id in enumerate(self.audio_speaker.values()):\n",
        "            self.speaker_ids[spk_id] = i\n",
        "        \n",
        "        \n",
        "        self.num_silence = 0\n",
        "        \n",
        "        if self.add_noise or self.add_silence_class:\n",
        "            self.background_noises_names = []\n",
        "            self.background_noises = []\n",
        "            for f in os.listdir(self.database_path + '/_background_noise_/'):\n",
        "                if f.endswith(\".wav\"):\n",
        "                    self.background_noises_names.append('_background_noise_/' + f)\n",
        "                    print(self.background_noises_names[-1])\n",
        "                    self.background_noises.append( self.load_audio(self.background_noises_names[-1]))\n",
        "\n",
        "                    \n",
        "        if self.add_silence_class:  #Adding the same amount of samples as the \"yes\" class\n",
        "            self.num_silence = sum(1 for i in self.audio_class.values() if i == 'yes')\n",
        "            \n",
        "        self.seeded = False\n",
        "        \n",
        "        \n",
        "    \n",
        "    def read_list_data(self):\n",
        "        with open(self.list_path, 'r') as stream:\n",
        "            for line in stream:\n",
        "                file_path = line.strip()\n",
        "                file_class, file_name = file_path.split('/')\n",
        "                identity = file_name.split('_')[0]\n",
        "                self.audio_class[file_path] = file_class\n",
        "                self.audio_speaker[file_path] = identity\n",
        "                self.audios.append(file_path)\n",
        "                \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.audio_class) + self.num_silence\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        if not self.seeded:\n",
        "            self.seeded = True\n",
        "            np.random.seed(index)\n",
        "            \n",
        "        if index >= len(self.audios):\n",
        "            spk_id=-1\n",
        "            length = 16000\n",
        "            audio = self.get_silence_chunk(length)\n",
        "            target_id = self.target_class['silence']\n",
        "            audio_id = 'random_silence/randomchunk.wav' \n",
        "        else:\n",
        "          \n",
        "            audio_id = self.audios[index]\n",
        "            audio = self.load_audio(audio_id)\n",
        "      \n",
        "            spk_id = self.speaker_ids[self.audio_speaker[audio_id]]\n",
        "            target = self.audio_class[audio_id]\n",
        "      \n",
        "            if target not in self.target_class_names:\n",
        "                target = 'unknown'\n",
        "            target_id = self.target_class[target]\n",
        "            \n",
        "        audio -= audio.mean()\n",
        "        #max_val = np.abs(audio[np.argpartition(np.abs(audio),-10)[-10:]]).mean()\n",
        "        max_val = np.abs(audio).max()\n",
        "        audio /= max_val + 0.001    \n",
        "            \n",
        "        sample = torch.FloatTensor(audio)\n",
        "        if self.add_noise:\n",
        "            alpha = np.random.uniform(low=0.90, high=1.00)\n",
        "            beta = 1.0 - alpha\n",
        "            silence_chunk = self.get_silence_chunk(len(audio))\n",
        "            max_val = np.abs(silence_chunk).max()\n",
        "            silence_chunk /= max_val + 0.001    \n",
        "            sample_noise = torch.FloatTensor(audio * alpha + beta * silence_chunk)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            sample = sample.view(1,-1)\n",
        "            sample = self.transform(sample)\n",
        "            if self.add_noise:\n",
        "                sample_noise = sample_noise.view(1,-1)\n",
        "                sample_noise = self.transform(sample_noise)\n",
        "              \n",
        "            \n",
        "        if self.add_noise:\n",
        "            return sample_noise,  target_id\n",
        "        else:\n",
        "            return sample, target_id\n",
        "        \n",
        "    def load_audio(self, audio_name):\n",
        "        \n",
        "        audio_path = self.database_path + audio_name\n",
        "        fs, audio = wavfile.read(audio_path)\n",
        "        audio = audio / 2**15\n",
        "        #audio, fs = librosa.load(audio_path)\n",
        "\n",
        "        return audio\n",
        "                 \n",
        "    \n",
        "    \n",
        "    def get_silence_chunk(self, length):\n",
        "        i = np.random.randint(0, len(self.background_noises))\n",
        "        silence = self.background_noises[i]\n",
        "        max_start = silence.shape[0] - length -1\n",
        "        random_start = np.random.randint(0, max_start)\n",
        "        #print(\"Starting at\", random_start )\n",
        "        chunk = silence[random_start:(random_start + length)]\n",
        "        return chunk\n",
        "      \n",
        "    def get_class_weights(self):\n",
        "        class_ids = []\n",
        "        for target in self.audio_class.values():\n",
        "            if target not in self.target_class_names:\n",
        "                target = 'unknown'\n",
        "            target_id = self.target_class[target]\n",
        "            class_ids.append(target_id)\n",
        "        for jj in range(self.num_silence):\n",
        "            class_ids.append(self.target_class['silence'])\n",
        "        class_ids.append(self.target_class['unknown'])\n",
        "        from sklearn.utils import class_weight\n",
        "        #print(np.unique(class_ids))\n",
        "        class_weight = class_weight.compute_class_weight('balanced', np.unique(class_ids),class_ids)\n",
        "        class_weight = torch.from_numpy(class_weight).float()\n",
        "        return class_weight\n",
        "      \n",
        "    def get_n_classes(self):\n",
        "        return len(self.target_class_names)\n",
        "     \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myp0dMin9tIV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "e2a6e077-5d5b-4051-9658-33c187a933ac"
      },
      "source": [
        "train_loader = data.DataLoader(\n",
        "                    AudioReader('gcommands/training_list.txt',add_silence_class=True), \n",
        "                        batch_size=50, shuffle=True, num_workers=2, pin_memory=True, \n",
        "                    )\n",
        "\n",
        "train_loader_noise = data.DataLoader(\n",
        "                    AudioReader('gcommands/training_list.txt',add_silence_class=True, add_noise=True), \n",
        "                        batch_size=50, shuffle=True, num_workers=2, pin_memory=True, \n",
        "                    )\n",
        "\n",
        "valid_loader = data.DataLoader(\n",
        "                    AudioReader('gcommands/validation_list.txt'), \n",
        "                        batch_size=50, shuffle=False, num_workers=2, pin_memory=True, \n",
        "                    )\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "                    AudioReader('gcommands/testing_list.txt'), \n",
        "                        batch_size=50, shuffle=False, num_workers=2, pin_memory=True, \n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_background_noise_/doing_the_dishes.wav\n",
            "_background_noise_/white_noise.wav\n",
            "_background_noise_/exercise_bike.wav\n",
            "_background_noise_/running_tap.wav\n",
            "_background_noise_/pink_noise.wav\n",
            "_background_noise_/dude_miaowing.wav\n",
            "_background_noise_/doing_the_dishes.wav\n",
            "_background_noise_/white_noise.wav\n",
            "_background_noise_/exercise_bike.wav\n",
            "_background_noise_/running_tap.wav\n",
            "_background_noise_/pink_noise.wav\n",
            "_background_noise_/dude_miaowing.wav\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:116: WavFileWarning: Chunk (non-data) not understood, skipping it.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g3Prx-aHO6T"
      },
      "source": [
        "\n",
        "for batch in train_loader:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-O7ZJPqJWB6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "7c086bbe-ca48-4656-ee5c-458e2ab54d1d"
      },
      "source": [
        "X, target = batch\n",
        "print(X.shape)\n",
        "print(target)\n",
        "train_loader.dataset.target_class_idx_to_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 1, 16000])\n",
            "tensor([10,  0,  7,  0,  0,  6,  0,  3,  0,  0,  0, 10, 10,  1,  0,  0,  0,  0,\n",
            "         2,  0,  0,  6,  0,  0,  0,  0,  2,  1,  0,  1,  6,  0,  0,  6, 11,  0,\n",
            "         0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0, 10,  4,  9])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'unknown',\n",
              " 1: 'silence',\n",
              " 2: 'yes',\n",
              " 3: 'no',\n",
              " 4: 'up',\n",
              " 5: 'down',\n",
              " 6: 'left',\n",
              " 7: 'right',\n",
              " 8: 'on',\n",
              " 9: 'off',\n",
              " 10: 'stop',\n",
              " 11: 'go'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5FtsCWQP2T7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e0214662-2945-4a5a-f3ae-d7d4f815adfc"
      },
      "source": [
        "train_loader.dataset.get_class_weights()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1356, 2.3723, 2.3723, 2.3812, 2.3941, 2.3954, 2.3994, 2.3825, 2.3672,\n",
              "        2.3994, 2.3408, 2.3710])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulS4YXFUwxCv"
      },
      "source": [
        "# LeNet Audio classification model\n",
        "\n",
        "Raw audio is converted into mel spectrogram and treated as a 2D image\n",
        "\n",
        "we use poutyne for training to reduce boilerplate code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WREbkaZkMi0j"
      },
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, num_classes=31):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.mels = nn.Sequential(\n",
        "            MelSpectrogram(),\n",
        "            nn.InstanceNorm2d(1) # Normalization\n",
        "        )\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 20, kernel_size=5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(20, 20, kernel_size=5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(20*7*22, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Compute mel filterbanks from raw audio\n",
        "        bs=x.shape[0]\n",
        "        debug=False\n",
        "        if debug:\n",
        "          print(x.shape) #bs, 1, 16000\n",
        "\n",
        "        x = self.mels(x)\n",
        "        if debug:\n",
        "          print(x.shape) #bs, 1, 40, 101\n",
        "        \n",
        "\n",
        "        ## Extract features\n",
        "        #bs, 20, ((40-5+1)/2 -5 +1)/2, int((int((40-5+1)/2) -5 +1)/2)\n",
        "        bs, 20, 7, 22\n",
        "        x=self.features(x)\n",
        "        \n",
        "        if debug:\n",
        "          print(x.shape)\n",
        "        \n",
        "        ## Flatten features\n",
        "        #reshape -> pytorch view(bs, c*height*width)  \n",
        "        #bs, features\n",
        "\n",
        "        x = x.view(bs,-1) #bs, 20*7*22\n",
        "\n",
        "        ## Classify\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        if debug:\n",
        "          raise Exception('In debug mode!')\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc0JM6L9nGrC"
      },
      "source": [
        "## Task description\n",
        "\n",
        "1. Adapt the shape of the features to be classified using either x.view(?,?) or nn.Flatten() in the classifier definition\n",
        "\n",
        "2. Compute the correct values for the first linear layer of the classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-tVvdboG4Vj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e808cf6-1ec0-46bd-b542-7d9025957565"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from poutyne.framework import Model\n",
        "from torchsummary import summary\n",
        "cuda_device = 0\n",
        "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
        "                      \n",
        "mymodel = LeNet(num_classes = train_loader.dataset.get_n_classes())\n",
        "print(mymodel.to(device))\n",
        "summary(mymodel, input_size=(1, 16000))\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Optimizer and loss function\n",
        "#optimizer = optim.SGD(mymodel.parameters(), lr=learning_rate, weight_decay=0.001)\n",
        "#optimizer = optim.Adam(mymodel.parameters(), lr=learning_rate)\n",
        "optimizer = optim.Adam( filter(lambda p: p.requires_grad, mymodel.parameters()), lr=learning_rate )\n",
        "#loss_function = nn.CrossEntropyLoss(weight=train_loader.dataset.get_class_weights())\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "model = Model(mymodel, optimizer, loss_function, batch_metrics=['accuracy'], epoch_metrics=['f1'])\n",
        "\n",
        "# Send model on GPU\n",
        "model.to(device)\n",
        "\n",
        "model.fit_generator(train_loader, valid_loader, epochs=10)\n",
        "\n",
        "\n",
        " # Test\n",
        "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (mels): Sequential(\n",
            "    (0): MelSpectrogram(\n",
            "      (norm): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    )\n",
            "    (1): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout2d(p=0.5, inplace=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=3080, out_features=1000, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=1000, out_features=12, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "    InstanceNorm2d-1           [-1, 1, 40, 101]               0\n",
            "            Conv2d-2           [-1, 20, 36, 97]             520\n",
            "         MaxPool2d-3           [-1, 20, 18, 48]               0\n",
            "              ReLU-4           [-1, 20, 18, 48]               0\n",
            "            Conv2d-5           [-1, 20, 14, 44]          10,020\n",
            "         MaxPool2d-6            [-1, 20, 7, 22]               0\n",
            "              ReLU-7            [-1, 20, 7, 22]               0\n",
            "         Dropout2d-8            [-1, 20, 7, 22]               0\n",
            "            Linear-9                 [-1, 1000]       3,081,000\n",
            "             ReLU-10                 [-1, 1000]               0\n",
            "           Linear-11                   [-1, 12]          12,012\n",
            "================================================================\n",
            "Total params: 3,103,552\n",
            "Trainable params: 3,103,552\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 1.01\n",
            "Params size (MB): 11.84\n",
            "Estimated Total Size (MB): 12.91\n",
            "----------------------------------------------------------------\n",
            "Epoch 1/10 24.30s Step 1059/1059: loss: 0.864733, acc: 74.108560, fscore_micro: 0.741086, val_loss: 0.457533, val_acc: 85.834069, val_fscore_micro: 0.858341\n",
            "Epoch 2/10 23.61s Step 1059/1059: loss: 0.433793, acc: 86.201556, fscore_micro: 0.862016, val_loss: 0.300405, val_acc: 90.629597, val_fscore_micro: 0.906296\n",
            "Epoch 3/10 23.53s Step 1059/1059: loss: 0.340886, acc: 88.953313, fscore_micro: 0.889533, val_loss: 0.281396, val_acc: 90.938511, val_fscore_micro: 0.909385\n",
            "Epoch 4/10 23.58s Step 1059/1059: loss: 0.285870, acc: 90.836292, fscore_micro: 0.908363, val_loss: 0.260385, val_acc: 91.762283, val_fscore_micro: 0.917623\n",
            "Epoch 5/10 23.40s Step 1059/1059: loss: 0.258991, acc: 91.697515, fscore_micro: 0.916975, val_loss: 0.237818, val_acc: 92.674316, val_fscore_micro: 0.926743\n",
            "Epoch 6/10 23.26s Step 1059/1059: loss: 0.225332, acc: 92.628617, fscore_micro: 0.926286, val_loss: 0.266832, val_acc: 91.526920, val_fscore_micro: 0.915269\n",
            "Epoch 7/10 23.40s Step 1059/1059: loss: 0.212540, acc: 92.959130, fscore_micro: 0.929591, val_loss: 0.229879, val_acc: 92.880259, val_fscore_micro: 0.928803\n",
            "Epoch 8/10 23.59s Step 1059/1059: loss: 0.190172, acc: 93.848682, fscore_micro: 0.938487, val_loss: 0.234707, val_acc: 93.380406, val_fscore_micro: 0.933804\n",
            "Epoch 9/10 23.26s Step 1059/1059: loss: 0.176417, acc: 94.235854, fscore_micro: 0.942359, val_loss: 0.246329, val_acc: 93.071492, val_fscore_micro: 0.930715\n",
            "Epoch 10/10 23.29s Step 1059/1059: loss: 0.162125, acc: 94.702349, fscore_micro: 0.947024, val_loss: 0.209143, val_acc: 93.718741, val_fscore_micro: 0.937187\n",
            "Test:\n",
            "\tLoss: 0.21579057339820043\n",
            "\tAccuracy: [93.54791514  0.93547922]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfus8c6SuRQn"
      },
      "source": [
        "## Check performance with data augmentation\n",
        "\n",
        "instead of training with train_loader we will train with train_loader_noise and evaluate on clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HW8RiwAt2zE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "e8c2f08e-07fc-4209-ed5b-43f78486cc0b"
      },
      "source": [
        "                      \n",
        "mymodel = LeNet(num_classes = train_loader.dataset.get_n_classes())\n",
        "learning_rate = 0.001\n",
        "\n",
        "optimizer = optim.Adam( filter(lambda p: p.requires_grad, mymodel.parameters()), lr=learning_rate )\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "model = Model(mymodel, optimizer, loss_function, batch_metrics=['accuracy'], epoch_metrics=['f1'])\n",
        "\n",
        "# Send model on GPU\n",
        "model.to(device)\n",
        "\n",
        "model.fit_generator(train_loader_noise, valid_loader, epochs=10)\n",
        "\n",
        "\n",
        " # Test\n",
        "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20 30.33s Step 1059/1059: loss: 0.887520, acc: 72.958374, fscore_micro: 0.729584, val_loss: 0.466242, val_acc: 85.672257, val_fscore_micro: 0.856723\n",
            "Epoch 2/20 30.24s Step 1059/1059: loss: 0.511687, acc: 83.487573, fscore_micro: 0.834876, val_loss: 0.352559, val_acc: 89.644013, val_fscore_micro: 0.896440\n",
            "Epoch 3/20 30.57s Step 1059/1059: loss: 0.416066, acc: 86.628390, fscore_micro: 0.866284, val_loss: 0.316556, val_acc: 90.688438, val_fscore_micro: 0.906884\n",
            "Epoch 4/20 30.21s Step 1059/1059: loss: 0.366843, acc: 88.165748, fscore_micro: 0.881657, val_loss: 0.281563, val_acc: 91.644601, val_fscore_micro: 0.916446\n",
            "Epoch 5/20 30.27s Step 1059/1059: loss: 0.333011, acc: 89.161064, fscore_micro: 0.891611, val_loss: 0.253941, val_acc: 92.233010, val_fscore_micro: 0.922330\n",
            "Epoch 6/20 30.43s Step 1059/1059: loss: 0.302482, acc: 90.299917, fscore_micro: 0.902999, val_loss: 0.256760, val_acc: 92.174169, val_fscore_micro: 0.921742\n",
            "Epoch 7/20 30.52s Step 1059/1059: loss: 0.282184, acc: 91.011936, fscore_micro: 0.910119, val_loss: 0.228694, val_acc: 92.939100, val_fscore_micro: 0.929391\n",
            "Epoch 8/20 30.74s Step 1059/1059: loss: 0.259259, acc: 91.474654, fscore_micro: 0.914747, val_loss: 0.229100, val_acc: 92.689026, val_fscore_micro: 0.926890\n",
            "Epoch 9/20 30.54s Step 1059/1059: loss: 0.249316, acc: 91.820277, fscore_micro: 0.918203, val_loss: 0.232309, val_acc: 93.071492, val_fscore_micro: 0.930715\n",
            "Epoch 10/20 30.61s Step 1059/1059: loss: 0.231990, acc: 92.460527, fscore_micro: 0.924605, val_loss: 0.222644, val_acc: 93.233304, val_fscore_micro: 0.932333\n",
            "Epoch 11/20 30.89s Step 1059/1059: loss: 0.221913, acc: 92.813704, fscore_micro: 0.928137, val_loss: 0.221917, val_acc: 93.453957, val_fscore_micro: 0.934540\n",
            "Epoch 12/20 30.43s Step 1059/1059: loss: 0.209316, acc: 93.113999, fscore_micro: 0.931140, val_loss: 0.222446, val_acc: 93.218594, val_fscore_micro: 0.932186\n",
            "Epoch 13/20 30.77s Step 1059/1059: loss: 0.198784, acc: 93.576717, fscore_micro: 0.935767, val_loss: 0.225604, val_acc: 93.336275, val_fscore_micro: 0.933363\n",
            "Epoch 14/20 30.30s Step 1059/1059: loss: 0.191546, acc: 93.867568, fscore_micro: 0.938676, val_loss: 0.238238, val_acc: 92.527214, val_fscore_micro: 0.925272\n",
            "Epoch 15/20 29.79s Step 1059/1059: loss: 0.186679, acc: 93.969555, fscore_micro: 0.939695, val_loss: 0.231016, val_acc: 92.924390, val_fscore_micro: 0.929244\n",
            "Epoch 16/20 29.31s Step 1059/1059: loss: 0.174081, acc: 94.428496, fscore_micro: 0.944285, val_loss: 0.218054, val_acc: 93.645190, val_fscore_micro: 0.936452\n",
            "Epoch 17/20 29.07s Step 1059/1059: loss: 0.174045, acc: 94.388834, fscore_micro: 0.943888, val_loss: 0.235881, val_acc: 92.880259, val_fscore_micro: 0.928803\n",
            "Epoch 18/20 29.46s Step 1059/1059: loss: 0.162350, acc: 94.796782, fscore_micro: 0.947968, val_loss: 0.246564, val_acc: 92.689026, val_fscore_micro: 0.926890\n",
            "Epoch 19/20 29.29s Step 1059/1059: loss: 0.157485, acc: 94.955428, fscore_micro: 0.949554, val_loss: 0.219716, val_acc: 93.453957, val_fscore_micro: 0.934540\n",
            "Epoch 20/20 29.39s Step 1059/1059: loss: 0.157406, acc: 95.023419, fscore_micro: 0.950234, val_loss: 0.239413, val_acc: 92.836128, val_fscore_micro: 0.928361\n",
            "Test:\n",
            "\tLoss: 0.23573387786371316\n",
            "\tAccuracy: [93.26993418  0.93269932]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqApa1RVuKh6"
      },
      "source": [
        "# VGG Audio classification model\n",
        "\n",
        "Let's borrow a successful image classification model and use it for audio mel-spectrograms \"images\" training the model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDkfbFbweeZO"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, vgg_name, num_classes=12):\n",
        "        super(VGG, self).__init__()\n",
        "        self.mels = nn.Sequential(\n",
        "            MelSpectrogram(),\n",
        "            nn.InstanceNorm2d(1) # Normalization\n",
        "          )\n",
        "        self.features = make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1 * 3 * 512, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mels(x)\n",
        "        x = self.features(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def make_layers(cfg, batch_norm=True):\n",
        "    layers = []\n",
        "    in_channels = 1\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIB83n9CvJVw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b03bb06c-47ac-4f53-aed9-ebc8b208a03a"
      },
      "source": [
        "#mymodel = LeNet(num_classes = train_loader.dataset.get_n_classes())\n",
        "#mymodel = TDNN(num_classes = train_loader.dataset.get_n_classes())\n",
        "mymodel = VGG('VGG11',num_classes = train_loader.dataset.get_n_classes())\n",
        "#mymodel = MyVGG(num_classes = train_loader.dataset.get_n_classes())\n",
        "print(mymodel.to(device))\n",
        "summary(mymodel, input_size=(1, 16000))\n",
        "learning_rate = 0.001\n",
        "\n",
        "optimizer = optim.Adam( filter(lambda p: p.requires_grad, mymodel.parameters()), lr=learning_rate )\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "model = Model(mymodel, optimizer, loss_function, batch_metrics=['accuracy'], epoch_metrics=['f1'])\n",
        "model.to(device)\n",
        "model.fit_generator(train_loader, valid_loader, epochs=10)\n",
        "\n",
        " # Test\n",
        "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (mels): Sequential(\n",
            "    (0): MelSpectrogram(\n",
            "      (norm): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    )\n",
            "    (1): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=12, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "    InstanceNorm2d-1           [-1, 1, 40, 101]               0\n",
            "            Conv2d-2          [-1, 64, 40, 101]             640\n",
            "       BatchNorm2d-3          [-1, 64, 40, 101]             128\n",
            "              ReLU-4          [-1, 64, 40, 101]               0\n",
            "         MaxPool2d-5           [-1, 64, 20, 50]               0\n",
            "            Conv2d-6          [-1, 128, 20, 50]          73,856\n",
            "       BatchNorm2d-7          [-1, 128, 20, 50]             256\n",
            "              ReLU-8          [-1, 128, 20, 50]               0\n",
            "         MaxPool2d-9          [-1, 128, 10, 25]               0\n",
            "           Conv2d-10          [-1, 256, 10, 25]         295,168\n",
            "      BatchNorm2d-11          [-1, 256, 10, 25]             512\n",
            "             ReLU-12          [-1, 256, 10, 25]               0\n",
            "           Conv2d-13          [-1, 256, 10, 25]         590,080\n",
            "      BatchNorm2d-14          [-1, 256, 10, 25]             512\n",
            "             ReLU-15          [-1, 256, 10, 25]               0\n",
            "        MaxPool2d-16           [-1, 256, 5, 12]               0\n",
            "           Conv2d-17           [-1, 512, 5, 12]       1,180,160\n",
            "      BatchNorm2d-18           [-1, 512, 5, 12]           1,024\n",
            "             ReLU-19           [-1, 512, 5, 12]               0\n",
            "           Conv2d-20           [-1, 512, 5, 12]       2,359,808\n",
            "      BatchNorm2d-21           [-1, 512, 5, 12]           1,024\n",
            "             ReLU-22           [-1, 512, 5, 12]               0\n",
            "        MaxPool2d-23            [-1, 512, 2, 6]               0\n",
            "           Conv2d-24            [-1, 512, 2, 6]       2,359,808\n",
            "      BatchNorm2d-25            [-1, 512, 2, 6]           1,024\n",
            "             ReLU-26            [-1, 512, 2, 6]               0\n",
            "           Conv2d-27            [-1, 512, 2, 6]       2,359,808\n",
            "      BatchNorm2d-28            [-1, 512, 2, 6]           1,024\n",
            "             ReLU-29            [-1, 512, 2, 6]               0\n",
            "        MaxPool2d-30            [-1, 512, 1, 3]               0\n",
            "           Linear-31                 [-1, 4096]       6,295,552\n",
            "             ReLU-32                 [-1, 4096]               0\n",
            "          Dropout-33                 [-1, 4096]               0\n",
            "           Linear-34                 [-1, 4096]      16,781,312\n",
            "             ReLU-35                 [-1, 4096]               0\n",
            "          Dropout-36                 [-1, 4096]               0\n",
            "           Linear-37                   [-1, 12]          49,164\n",
            "================================================================\n",
            "Total params: 32,350,860\n",
            "Trainable params: 32,350,860\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 14.59\n",
            "Params size (MB): 123.41\n",
            "Estimated Total Size (MB): 138.06\n",
            "----------------------------------------------------------------\n",
            "Epoch 1/10 71.46s Step 1059/1059: loss: 0.718693, acc: 78.935937, fscore_micro: 0.789359, val_loss: 0.365053, val_acc: 89.644013, val_fscore_micro: 0.896440\n",
            "Epoch 2/10 71.32s Step 1059/1059: loss: 0.266125, acc: 93.072448, fscore_micro: 0.930725, val_loss: 0.237630, val_acc: 94.307149, val_fscore_micro: 0.943071\n",
            "Epoch 3/10 71.17s Step 1059/1059: loss: 0.263902, acc: 93.905341, fscore_micro: 0.939053, val_loss: 0.301550, val_acc: 91.556340, val_fscore_micro: 0.915563\n",
            "Epoch 4/10 71.10s Step 1059/1059: loss: 0.175300, acc: 95.535242, fscore_micro: 0.955352, val_loss: 0.180493, val_acc: 95.160341, val_fscore_micro: 0.951603\n",
            "Epoch 5/10 71.21s Step 1059/1059: loss: 0.106572, acc: 97.104707, fscore_micro: 0.971047, val_loss: 0.130210, val_acc: 96.410709, val_fscore_micro: 0.964107\n",
            "Epoch 6/10 71.24s Step 1059/1059: loss: 0.136553, acc: 96.523004, fscore_micro: 0.965230, val_loss: 0.151312, val_acc: 95.689909, val_fscore_micro: 0.956899\n",
            "Epoch 7/10 71.30s Step 1059/1059: loss: 0.086538, acc: 97.614641, fscore_micro: 0.976146, val_loss: 0.156364, val_acc: 96.293027, val_fscore_micro: 0.962930\n",
            "Epoch 8/10 71.26s Step 1059/1059: loss: 0.093268, acc: 97.682632, fscore_micro: 0.976826, val_loss: 0.250532, val_acc: 94.071786, val_fscore_micro: 0.940718\n",
            "Epoch 9/10 71.22s Step 1059/1059: loss: 0.162761, acc: 96.683539, fscore_micro: 0.966835, val_loss: 0.158606, val_acc: 96.278317, val_fscore_micro: 0.962783\n",
            "Epoch 10/10 71.31s Step 1059/1059: loss: 0.065764, acc: 98.207675, fscore_micro: 0.982077, val_loss: 0.136716, val_acc: 96.807885, val_fscore_micro: 0.968079\n",
            "Test:\n",
            "\tLoss: 0.1284760007099355\n",
            "\tAccuracy: [97.13240673  0.97132409]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xCi5wSduszZ"
      },
      "source": [
        "## VGG fine tuning\n",
        "\n",
        "Use a pretrained VGG, freezing its parameters and replacing the last linear layer with a new one with the needed num_classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEsZgiTzgTsO"
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MyVGG(nn.Module):\n",
        "    def __init__(self, num_classes=12):\n",
        "          super(MyVGG, self).__init__()\n",
        "          self.origVGG = models.vgg11(pretrained=True)\n",
        "          for param in self.origVGG.parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "          in_features = self.origVGG.classifier[6].in_features\n",
        "          self.origVGG.classifier[6] = nn.Linear(in_features, num_classes)\n",
        "          conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "          self.origVGG.features[0] = conv1\n",
        "          self.mels = nn.Sequential(\n",
        "            MelSpectrogram(),\n",
        "            nn.InstanceNorm2d(1) # Normalization\n",
        "          )\n",
        "          \n",
        "    def forward(self, x):\n",
        "        x = self.mels(x)\n",
        "        x = self.origVGG(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBZy5VE1jDvT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e02de92-7f93-4673-f5db-a7dd65df0a25"
      },
      "source": [
        "#mymodel = LeNet(num_classes = train_loader.dataset.get_n_classes())\n",
        "#mymodel = TDNN(num_classes = train_loader.dataset.get_n_classes())\n",
        "#mymodel = VGG('VGG11',num_classes = train_loader.dataset.get_n_classes())\n",
        "mymodel = MyVGG(num_classes = train_loader.dataset.get_n_classes())\n",
        "print(mymodel.to(device))\n",
        "summary(mymodel, input_size=(1, 16000))\n",
        "learning_rate = 0.001\n",
        "\n",
        "optimizer = optim.Adam( filter(lambda p: p.requires_grad, mymodel.parameters()), lr=learning_rate )\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "model = Model(mymodel, optimizer, loss_function, batch_metrics=['accuracy'], epoch_metrics=['f1'])\n",
        "model.to(device)\n",
        "model.fit_generator(train_loader, valid_loader, epochs=10)\n",
        "\n",
        " # Test\n",
        "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyVGG(\n",
            "  (origVGG): VGG(\n",
            "    (features): Sequential(\n",
            "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): ReLU(inplace=True)\n",
            "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): ReLU(inplace=True)\n",
            "      (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (9): ReLU(inplace=True)\n",
            "      (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (12): ReLU(inplace=True)\n",
            "      (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (14): ReLU(inplace=True)\n",
            "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (17): ReLU(inplace=True)\n",
            "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (19): ReLU(inplace=True)\n",
            "      (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "      (4): ReLU(inplace=True)\n",
            "      (5): Dropout(p=0.5, inplace=False)\n",
            "      (6): Linear(in_features=4096, out_features=12, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (mels): Sequential(\n",
            "    (0): MelSpectrogram(\n",
            "      (norm): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    )\n",
            "    (1): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "    InstanceNorm2d-1           [-1, 1, 40, 101]               0\n",
            "            Conv2d-2          [-1, 64, 40, 101]             640\n",
            "              ReLU-3          [-1, 64, 40, 101]               0\n",
            "         MaxPool2d-4           [-1, 64, 20, 50]               0\n",
            "            Conv2d-5          [-1, 128, 20, 50]          73,856\n",
            "              ReLU-6          [-1, 128, 20, 50]               0\n",
            "         MaxPool2d-7          [-1, 128, 10, 25]               0\n",
            "            Conv2d-8          [-1, 256, 10, 25]         295,168\n",
            "              ReLU-9          [-1, 256, 10, 25]               0\n",
            "           Conv2d-10          [-1, 256, 10, 25]         590,080\n",
            "             ReLU-11          [-1, 256, 10, 25]               0\n",
            "        MaxPool2d-12           [-1, 256, 5, 12]               0\n",
            "           Conv2d-13           [-1, 512, 5, 12]       1,180,160\n",
            "             ReLU-14           [-1, 512, 5, 12]               0\n",
            "           Conv2d-15           [-1, 512, 5, 12]       2,359,808\n",
            "             ReLU-16           [-1, 512, 5, 12]               0\n",
            "        MaxPool2d-17            [-1, 512, 2, 6]               0\n",
            "           Conv2d-18            [-1, 512, 2, 6]       2,359,808\n",
            "             ReLU-19            [-1, 512, 2, 6]               0\n",
            "           Conv2d-20            [-1, 512, 2, 6]       2,359,808\n",
            "             ReLU-21            [-1, 512, 2, 6]               0\n",
            "        MaxPool2d-22            [-1, 512, 1, 3]               0\n",
            "AdaptiveAvgPool2d-23            [-1, 512, 7, 7]               0\n",
            "           Linear-24                 [-1, 4096]     102,764,544\n",
            "             ReLU-25                 [-1, 4096]               0\n",
            "          Dropout-26                 [-1, 4096]               0\n",
            "           Linear-27                 [-1, 4096]      16,781,312\n",
            "             ReLU-28                 [-1, 4096]               0\n",
            "          Dropout-29                 [-1, 4096]               0\n",
            "           Linear-30                   [-1, 12]          49,164\n",
            "              VGG-31                   [-1, 12]               0\n",
            "================================================================\n",
            "Total params: 128,814,348\n",
            "Trainable params: 49,804\n",
            "Non-trainable params: 128,764,544\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 10.29\n",
            "Params size (MB): 491.39\n",
            "Estimated Total Size (MB): 501.74\n",
            "----------------------------------------------------------------\n",
            "Epoch 1/10 52.88s Step 1059/1059: loss: 1.155943, acc: 66.253683, fscore_micro: 0.662537, val_loss: 0.974924, val_acc: 69.035010, val_fscore_micro: 0.690350\n",
            "Epoch 2/10 52.38s Step 1059/1059: loss: 0.990954, acc: 69.326509, fscore_micro: 0.693265, val_loss: 0.913683, val_acc: 71.977052, val_fscore_micro: 0.719770\n",
            "Epoch 3/10 52.37s Step 1059/1059: loss: 0.936132, acc: 70.875198, fscore_micro: 0.708752, val_loss: 0.839494, val_acc: 74.110033, val_fscore_micro: 0.741100\n",
            "Epoch 4/10 52.21s Step 1059/1059: loss: 0.900602, acc: 71.689205, fscore_micro: 0.716892, val_loss: 0.799373, val_acc: 75.213298, val_fscore_micro: 0.752133\n",
            "Epoch 5/10 51.97s Step 1059/1059: loss: 0.870829, acc: 72.523986, fscore_micro: 0.725240, val_loss: 0.765755, val_acc: 76.228303, val_fscore_micro: 0.762283\n",
            "Epoch 6/10 51.71s Step 1059/1059: loss: 0.850711, acc: 72.984815, fscore_micro: 0.729848, val_loss: 0.755625, val_acc: 76.743160, val_fscore_micro: 0.767432\n",
            "Epoch 7/10 51.84s Step 1059/1059: loss: 0.833301, acc: 73.385208, fscore_micro: 0.733852, val_loss: 0.728989, val_acc: 76.493086, val_fscore_micro: 0.764931\n",
            "Epoch 8/10 51.83s Step 1059/1059: loss: 0.805590, acc: 74.471179, fscore_micro: 0.744712, val_loss: 0.743267, val_acc: 75.316270, val_fscore_micro: 0.753163\n",
            "Epoch 9/10 51.85s Step 1059/1059: loss: 0.802726, acc: 74.525950, fscore_micro: 0.745260, val_loss: 0.749996, val_acc: 76.272433, val_fscore_micro: 0.762724\n",
            "Epoch 10/10 51.68s Step 1059/1059: loss: 0.790071, acc: 74.839465, fscore_micro: 0.748395, val_loss: 0.679912, val_acc: 77.861136, val_fscore_micro: 0.778611\n",
            "Test:\n",
            "\tLoss: 0.6900103190244771\n",
            "\tAccuracy: [77.79078293  0.77790785]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywxJY9LpvGGC"
      },
      "source": [
        "# TDNN Classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLvuuj0KwWab"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TDNN(nn.Module):\n",
        "    def __init__(self, num_classes=12):\n",
        "        super(TDNN, self).__init__()\n",
        "        self.tdnn = nn.Sequential(\n",
        "            nn.Conv1d(40, 450, stride=1, dilation=1, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=1, kernel_size=4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=3, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=3, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=3, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=3, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(450, 450, stride=1, dilation=3, kernel_size=3),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool1d(3, stride=3),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(9000, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        self.mels = nn.Sequential(\n",
        "            MelSpectrogram(),\n",
        "            nn.InstanceNorm2d(1) # Normalization\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=self.mels(x)\n",
        "        x.squeeze_(1)\n",
        "        x = self.tdnn(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}