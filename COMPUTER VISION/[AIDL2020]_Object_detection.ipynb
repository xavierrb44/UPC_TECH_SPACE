{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[AIDL2020] Object detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DZwK1xpCd9a"
      },
      "source": [
        "# Object detection\n",
        "\n",
        "**Notebook created by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2019).**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OdVfmDx8LoQ"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import time\n",
        "import torchvision\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"You should enable GPU runtime.\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = [20, 10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjV2crwuYCl4"
      },
      "source": [
        "def show_image(pil_im):\n",
        "    plt.imshow(np.asarray(pil_im))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjoZl4ksHQcN"
      },
      "source": [
        "For this lab we will train a Faster R-CNN (Region CNN). This kind of network is used to detect where a specific object is located in an image (giving its bounding box)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnE3PTxOKxHI"
      },
      "source": [
        "# Faster RCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsjBmHyMVUsF"
      },
      "source": [
        "We will use [Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.\n",
        "\n",
        "![Faster R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eKBqyYAVgww"
      },
      "source": [
        "We will use the TorchVision implementation of this architecture. First, we will use a pretrained Faster R-CNN to detect objects in an image, and then we will transfer learning to train in our own dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aETRGpLHdtOS"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0jQV36hAfwR"
      },
      "source": [
        "First we will make some simple modules that will be used by the Faster RCNN module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-AIYsMV8lu"
      },
      "source": [
        "This is the torchvision implementation of Faster R-CNN. We will go through each part of the network to understand how it works. \n",
        "\n",
        "**Note:** If you want to use this model outside of the lab, you don't need to use if like this, you can just get it by calling `torchvision.models.detection.fasterrcnn_resnet50_fpn` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jiqb4LT-1ZK"
      },
      "source": [
        "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
        "from torchvision.models.detection.roi_heads import RoIHeads\n",
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "from torchvision.models.detection.faster_rcnn import TwoMLPHead, FastRCNNPredictor\n",
        "from torchvision.ops import misc as misc_nn_ops\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "\n",
        "class FasterRCNN(nn.Module):\n",
        "    def __init__(self, backbone, rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000,\n",
        "                 rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000,\n",
        "                 rpn_nms_thresh=0.7,\n",
        "                 rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,\n",
        "                 rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n",
        "                 box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100,\n",
        "                 box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n",
        "                 box_batch_size_per_image=512, box_positive_fraction=0.25,\n",
        "                 bbox_reg_weights=None, num_classes=91):\n",
        "        super().__init__()\n",
        "        # First, set the backbone that we will use to extract the feature maps\n",
        "        self.backbone = backbone\n",
        "\n",
        "        out_channels = backbone.out_channels\n",
        "        anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
        "        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
        "\n",
        "        # Declare the anchor generator of the Region Proposal Network (RPN)\n",
        "        rpn_anchor_generator = AnchorGenerator(\n",
        "            anchor_sizes, aspect_ratios\n",
        "        )\n",
        "\n",
        "        # Declare the Head of the RPN that will classify the proposals\n",
        "        rpn_head = RPNHead(\n",
        "            out_channels, rpn_anchor_generator.num_anchors_per_location()[0]\n",
        "        )\n",
        "        rpn_pre_nms_top_n = {\"training\": rpn_pre_nms_top_n_train, \"testing\": rpn_pre_nms_top_n_test}\n",
        "        rpn_post_nms_top_n = {\"training\": rpn_post_nms_top_n_train, \"testing\": rpn_post_nms_top_n_test}\n",
        "\n",
        "        # Create the RPN combining the anchor generator and the head\n",
        "        self.rpn = RegionProposalNetwork(\n",
        "            rpn_anchor_generator, rpn_head,\n",
        "            rpn_fg_iou_thresh, rpn_bg_iou_thresh,\n",
        "            rpn_batch_size_per_image, rpn_positive_fraction,\n",
        "            rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh)\n",
        "        \n",
        "        # ROI Align\n",
        "        box_roi_pool = MultiScaleRoIAlign(\n",
        "            featmap_names=['0', '1', '2', '3'],\n",
        "            output_size=7,\n",
        "            sampling_ratio=2)\n",
        "\n",
        "        resolution = box_roi_pool.output_size[0]\n",
        "        representation_size = 1024\n",
        "\n",
        "        # Simple Head made of 2 Fully Connected layers\n",
        "        box_head = None\n",
        "\n",
        "        representation_size = 1024\n",
        "\n",
        "        # Predictor with 2 outputs (scores and Bounding Boxes deltas)\n",
        "        box_predictor = None\n",
        "\n",
        "        # Module that combines the last modules to compute the prediction of every box and match \n",
        "        # targets and proposalt in training \n",
        "        self.roi_heads = RoIHeads(\n",
        "            # Box\n",
        "            box_roi_pool, box_head, box_predictor,\n",
        "            box_fg_iou_thresh, box_bg_iou_thresh,\n",
        "            box_batch_size_per_image, box_positive_fraction,\n",
        "            bbox_reg_weights,\n",
        "            box_score_thresh, box_nms_thresh, box_detections_per_img)\n",
        "        min_size=800\n",
        "        max_size=1333\n",
        "        self.image_mean = [0.485, 0.456, 0.406]\n",
        "        self.image_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "        # Module that does the correct data augmentation\n",
        "        self.transform = GeneralizedRCNNTransform(min_size, max_size, self.image_mean, self.image_std)\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]])\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            images (list[Tensor]): images to be processed\n",
        "            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)\n",
        "\n",
        "        Returns:\n",
        "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
        "                During training, it returns a dict[Tensor] which contains the losses.\n",
        "                During testing, it returns list[BoxList] contains additional fields\n",
        "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
        "\n",
        "        \"\"\"\n",
        "        if self.training and targets is None:\n",
        "            raise ValueError(\"In training mode, targets should be passed\")\n",
        "        original_image_sizes = []\n",
        "        for img in images:\n",
        "            val = img.shape[-2:]\n",
        "            assert len(val) == 2\n",
        "            original_image_sizes.append((val[0], val[1]))\n",
        "\n",
        "        images, targets = self.transform(images, targets)\n",
        "        features = self.backbone(images.tensors)\n",
        "        if isinstance(features, torch.Tensor):\n",
        "            features = OrderedDict([('0', features)])\n",
        "        proposals, proposal_losses = self.rpn(images, features, targets)\n",
        "        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
        "        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
        "\n",
        "        losses = {}\n",
        "        losses.update(detector_losses)\n",
        "        losses.update(proposal_losses)\n",
        "\n",
        "        return detections, losses\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq1-da5nTlun"
      },
      "source": [
        "We can get a backbone for feature extraction and instantiate the model. Note that the model is missing 2 modules (box head and box predictor). We will add them in the following exercises."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMk_tqLk-1Wg"
      },
      "source": [
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "\n",
        "backbone = resnet_fpn_backbone(\"resnet50\", pretrained=True)\n",
        "model = FasterRCNN(backbone)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKVjSGPrAbuf"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O1KSwgcQBX7"
      },
      "source": [
        "Complete the code for `TwoMLPHead`. This module goes on top of the Region Proposal Network. It should have two Linear layers with intermidiate size of `representation_size` and output size of also `representation_size`. The input `x` for it will be of size `[batch, 256, 7, 7]`, you will have to flatten it. (`in_size=256*7*7`). Use `F.relu` after each linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eXYXAObAW2I"
      },
      "source": [
        "class TwoMLPHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard heads for FPN-based models\n",
        "\n",
        "    Arguments:\n",
        "        in_size (int): number of inputs\n",
        "        representation_size (int): size of the intermediate representation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_size, representation_size):\n",
        "        super(TwoMLPHead, self).__init__()\n",
        "\n",
        "        self.fc6 = ...\n",
        "        self.fc7 = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2umEw6pRvdh"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZXC_fTtRzp9"
      },
      "source": [
        "Now, complete the code for the predictor. Note that this predictor has 2 outputs, the predicted class (of size `num_classes`) and the bounding box deltas (of size `4*num_classes`). The 4 comes from the 4 bounding box sides. The input now will be of size `[batch, in_size]`. Don't add any activations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt-so-82A2ig"
      },
      "source": [
        "class FastRCNNPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard classification + bounding box regression layers\n",
        "    for Fast R-CNN.\n",
        "\n",
        "    Arguments:\n",
        "        in_size (int): number of input channels\n",
        "        num_classes (int): number of output classes (including background)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_size, num_classes):\n",
        "        super(FastRCNNPredictor, self).__init__()\n",
        "        self.cls_score = ...\n",
        "        self.bbox_pred = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = ...\n",
        "        bbox_deltas = ...\n",
        "\n",
        "        return scores, bbox_deltas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNOUaUF7TsmN"
      },
      "source": [
        "Now we can instantiate these 2 modules and put them in the model architecture. With this, the model will be complete and will be ready to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wYUj9LECJjs"
      },
      "source": [
        "in_size = 256*7*7\n",
        "representation_size = 1024\n",
        "num_classes = 91\n",
        "model.roi_heads.box_head = TwoMLPHead(in_size, representation_size)\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(representation_size, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsOEwfEoT18P"
      },
      "source": [
        "We will load pretrained weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ8MmCF8-1dT"
      },
      "source": [
        "url = \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\"\n",
        "state_dict =  torch.hub.load_state_dict_from_url(url)\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device).eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZoMghltZp0p"
      },
      "source": [
        "We will use the following image to try the detector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceSnyp5N-1lA"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "urlretrieve(\"https://farm1.staticflickr.com/76/152982198_da205ee3dc_z.jpg\", \"image.jpg\")\n",
        "im = Image.open(\"image.jpg\")\n",
        "show_image(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41BsHBRUcaL6"
      },
      "source": [
        "We will also get the list of COCO labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XQtj72ynoEd"
      },
      "source": [
        "a = urlretrieve(\"https://raw.githubusercontent.com/amikelive/coco-labels/master/coco-labels-2014_2017.txt\", \"labels.txt\")\n",
        "coco_labels = [\"background\"]\n",
        "with open(\"labels.txt\", \"r\") as f:\n",
        "    for line in f.readlines():\n",
        "        coco_labels.append(line.replace(\"\\n\", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYb64pUwao4s"
      },
      "source": [
        "## Preprocessing transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGG8YFNBZ4HR"
      },
      "source": [
        "We will convert the image to a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCalBgC6-1nQ"
      },
      "source": [
        "from torchvision.transforms.functional import to_tensor\n",
        "x = to_tensor(im).to(device)\n",
        "targets = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gc4MOvbZ7Eb"
      },
      "source": [
        "And now, we will apply the preprocess transformations (normalizing to imagenet values and resizing). This is already implemented in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HeypxbO-1py"
      },
      "source": [
        "transformed, _ = model.transform([x], targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdQOyyioatNn"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqzfFpffWF5Q"
      },
      "source": [
        "The first part of the network is a backbone for feature extraction. To do it, we will use a Feature Pyramidal Network with a ResNet50. The idea of these feature extraction network is to have semantically strong features of different scale.\n",
        "\n",
        "![FPN1](https://cdn-images-1.medium.com/freeze/max/1000/1*7KbOAExR3E05ZDv0CwXWjg.png?q=20)\n",
        "\n",
        "Here, the scale of the features are represented with the size, and the semantical meaning of the features is represented with the thickness of the blue outline. Feature Pyramid Networks achieve semantically storng features at different object scales (whch will help to detect small and big objects).\n",
        "\n",
        "\n",
        "The architecture of these feature extractor is the following one (1x1 and 3x3 represent convolutions):\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/968/1*edviRcl3vwlyx9TS_gRbmg.png\" alt=\"Kitten\"\n",
        "\ttitle=\"arch\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MyDgwlzZYFV"
      },
      "source": [
        "We can use the FPN from torchvision to get the feature maps. Then, we can take a look at the size of the maps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_VVQyk2-1sj"
      },
      "source": [
        "feature_maps = model.backbone(transformed.tensors)\n",
        "print({k: v.shape for k, v in feature_maps.items()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5S6pZhqa2VX"
      },
      "source": [
        "## Region proposal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOAotAUShs3_"
      },
      "source": [
        "Now, from the feature maps, we can generate the region proposals using the RPN. This will give us 1000 proposals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX8dlq0Bg7E8"
      },
      "source": [
        "proposals, proposal_losses = model.rpn(transformed, feature_maps, targets)\n",
        "proposals[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap7K-D7Ba6l0"
      },
      "source": [
        "We can take a look at some of the region of interests proposals (this are the first 50)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC2Pvn-fh8Bn"
      },
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "aux = transformed.tensors[0].cpu()\n",
        "aux = aux*torch.tensor(model.image_std).reshape(3,1,1)+torch.tensor(model.image_mean).reshape(3,1,1)\n",
        "trnasformed_im = to_pil_image(aux)\n",
        "draw = ImageDraw.Draw(trnasformed_im)\n",
        "for p in proposals[0][:50]:\n",
        "    coords = p.cpu().detach().tolist()\n",
        "    draw.rectangle(coords, width=2)\n",
        "show_image(trnasformed_im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvjHZuHYbJEe"
      },
      "source": [
        "We can see that we have a lot of proposals near objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41hqByiFbQL4"
      },
      "source": [
        "## ROI classifier and regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kRgfBdHbY2c"
      },
      "source": [
        "From those proposals, we can classify them (as either empty or one of the classes) and do regression to get the correct bounding box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOSQ2FTRlQSr"
      },
      "source": [
        "detections, detector_losses = model.roi_heads(feature_maps, proposals, transformed.image_sizes, targets)\n",
        "detections = model.transform.postprocess(detections, transformed.image_sizes, [[im.height, im.width]])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQNnUbU4bzdF"
      },
      "source": [
        "We can take a look at the shapes of detections (what are the meaning of these numbers?):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3ypO9EWlZLs"
      },
      "source": [
        "print({k: v.shape for k, v in detections.items()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crY39-OZcESX"
      },
      "source": [
        "We see that we got too many detections. This is because we haven't applied non maximum supression yet (this is necessary to discard overlapping detections). We can use `torchvision.ops.nms` to decide which detections to keep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmwni3kKmMc2"
      },
      "source": [
        "iou_threshold = 0.2\n",
        "scores_threshold = 0.6\n",
        "keep_idx = torchvision.ops.nms(detections[\"boxes\"], detections[\"scores\"], iou_threshold)\n",
        "boxes = [b for i, b in enumerate(detections[\"boxes\"]) if i in keep_idx]\n",
        "scores = [s for i, s in enumerate(detections[\"scores\"]) if i in keep_idx]\n",
        "labels = [l for i, l in enumerate(detections[\"labels\"]) if i in keep_idx]\n",
        "draw = ImageDraw.Draw(im)\n",
        "for box, score, label in zip(boxes, scores, labels):\n",
        "    if score > scores_threshold:\n",
        "        coords = box.cpu().tolist()\n",
        "        draw.rectangle(coords, width=3)\n",
        "        text = f\"{coco_labels[label.item()]} {score*100:.2f}%\"\n",
        "        draw.text([coords[0], coords[1]-15], text)\n",
        "show_image(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPeXajBclG3O"
      },
      "source": [
        "# Transfer Learning to Shapes Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUiLOk-bYw9d"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfAS-XSEY0FN"
      },
      "source": [
        "Now we will use an artificial \"shapes dataset\" made of randomly generated images with square, triangles and circles. Our objective will be to find and classify this shapes. \n",
        "\n",
        "The following code represents this datset. The code to generate the images is not important, but you can take a look at it if you are curious."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kawHbm6zN375"
      },
      "source": [
        "class ShapesDataset:\n",
        "    label2idx = {\"circle\": 1, \"square\": 2, \"triangle\": 3}\n",
        "\n",
        "    def random_shape(self, height, width):\n",
        "        \"\"\"Generates specifications of a random shape that lies within\n",
        "        the given height and width boundaries.\n",
        "        Returns a tuple of three valus:\n",
        "        * The shape name (square, circle, ...)\n",
        "        * Shape color: a tuple of 3 values, RGB.\n",
        "        * Shape dimensions: A tuple of values that define the shape size\n",
        "                            and location. Differs per shape type.\n",
        "        \"\"\"\n",
        "        # Shape\n",
        "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
        "        # Color\n",
        "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
        "        # Center x, y\n",
        "        buffer = 20\n",
        "        y = random.randint(buffer, height - buffer - 1)\n",
        "        x = random.randint(buffer, width - buffer - 1)\n",
        "        # Size\n",
        "        s = random.randint(buffer, height//4)\n",
        "        return shape, color, (x, y, s)\n",
        "\n",
        "    def random_image(self, height, width):\n",
        "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
        "        Returns the background color of the image and a list of shape\n",
        "        specifications that can be used to draw the image.\n",
        "        \"\"\"\n",
        "        # Pick random background color\n",
        "        bg_color = [random.randint(0, 255) for _ in range(3)]\n",
        "        # Generate a few random shapes and record their\n",
        "        # bounding boxes\n",
        "        shapes = []\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        N = random.randint(1, 4)\n",
        "        for _ in range(N):\n",
        "            shape, color, dims = self.random_shape(height, width)\n",
        "            shapes.append((shape, color, dims))\n",
        "            labels.append(self.label2idx[shape])\n",
        "            x, y, s = dims\n",
        "            boxes.append([x-s, y-s, x+s, y+s])\n",
        "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
        "        # shapes covering each other\n",
        "        keep_ixs = self.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
        "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
        "        labels = [l for i, l in enumerate(labels) if i in keep_ixs]\n",
        "        boxes = [b for i, b in enumerate(boxes) if i in keep_ixs]\n",
        "\n",
        "        im = Image.new('RGB', (height, width), tuple(bg_color))\n",
        "        draw = ImageDraw.Draw(im)\n",
        "        for shape, color, (x, y, s) in shapes:\n",
        "            if shape == \"circle\":\n",
        "                draw.ellipse((x-s, y-s, x+s, y+s), fill=color)\n",
        "            elif shape == \"triangle\":\n",
        "                points = [(x, y-s), (x-s/math.sin(math.radians(60)), y+s), (x+s/math.sin(math.radians(60)), y+s)]\n",
        "                draw.polygon(points, fill=color)\n",
        "            elif shape == \"square\":\n",
        "                draw.rectangle((x-s, y-s, x+s, y+s), fill=color)\n",
        "        del draw\n",
        "\n",
        "        return im, boxes, labels\n",
        "    \n",
        "    def non_max_suppression(self, boxes, scores, threshold):\n",
        "        \"\"\"Performs non-maximum suppression and returns indices of kept boxes.\n",
        "        boxes: [N, (x1, y1, x2, y2)]. Notice that (x2, y2) lays outside the box.\n",
        "        scores: 1-D array of box scores.\n",
        "        threshold: Float. IoU threshold to use for filtering.\n",
        "        \"\"\"\n",
        "        assert boxes.shape[0] > 0\n",
        "        if boxes.dtype.kind != \"f\":\n",
        "            boxes = boxes.astype(np.float32)\n",
        "\n",
        "        # Compute box areas\n",
        "        x1 = boxes[:, 0]\n",
        "        y1 = boxes[:, 1]\n",
        "        x2 = boxes[:, 2]\n",
        "        y2 = boxes[:, 3]\n",
        "        area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "        # Get indicies of boxes sorted by scores (highest first)\n",
        "        ixs = scores.argsort()[::-1]\n",
        "\n",
        "        pick = []\n",
        "        while len(ixs) > 0:\n",
        "            # Pick top box and add its index to the list\n",
        "            i = ixs[0]\n",
        "            pick.append(i)\n",
        "            # Compute IoU of the picked box with the rest\n",
        "            iou = self.compute_iou(boxes[i], boxes[ixs[1:]], area[i], area[ixs[1:]])\n",
        "            # Identify boxes with IoU over the threshold. This\n",
        "            # returns indices into ixs[1:], so add 1 to get\n",
        "            # indices into ixs.\n",
        "            remove_ixs = np.where(iou > threshold)[0] + 1\n",
        "            # Remove indices of the picked and overlapped boxes.\n",
        "            ixs = np.delete(ixs, remove_ixs)\n",
        "            ixs = np.delete(ixs, 0)\n",
        "        return np.array(pick, dtype=np.int32)\n",
        "\n",
        "    def compute_iou(self, box, boxes, box_area, boxes_area):\n",
        "        \"\"\"Calculates IoU of the given box with the array of the given boxes.\n",
        "        box: 1D vector [x1, y1, x2, y2]\n",
        "        boxes: [boxes_count, (x1, y1, x2, y2)]\n",
        "        box_area: float. the area of 'box'\n",
        "        boxes_area: array of length boxes_count.\n",
        "        Note: the areas are passed in rather than calculated here for\n",
        "        efficiency. Calculate once in the caller to avoid duplicate work.\n",
        "        \"\"\"\n",
        "        # Calculate intersection areas\n",
        "        x1 = np.maximum(box[0], boxes[:, 0])\n",
        "        x2 = np.minimum(box[2], boxes[:, 2])\n",
        "        y1 = np.maximum(box[1], boxes[:, 1])\n",
        "        y2 = np.minimum(box[3], boxes[:, 3])\n",
        "        intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
        "        union = box_area + boxes_area[:] - intersection[:]\n",
        "        iou = intersection / union\n",
        "        return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2larfuyN6ja"
      },
      "source": [
        "shapes_dataset = ShapesDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alZShTlrOKGI"
      },
      "source": [
        "im, boxes, labels = shapes_dataset.random_image(200, 200)\n",
        "draw = ImageDraw.Draw(im)\n",
        "for b in boxes:\n",
        "    draw.rectangle(b)\n",
        "del draw\n",
        "show_image(im)\n",
        "boxes, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnfHJb2UZ7xM"
      },
      "source": [
        "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PytorchDaset(Dataset):\n",
        "    def __init__(self, shapes_dataset, iterations=2000, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.shapes_dataset = shapes_dataset\n",
        "        self.iterations = iterations\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        im, boxes, labels = shapes_dataset.random_image(100, 100)\n",
        "        targets = {\"boxes\": torch.tensor(boxes, dtype=torch.float32, device=device), \n",
        "                   \"labels\": torch.tensor(labels, dtype=torch.int64, device=device)}\n",
        "        return to_tensor(im).to(device), targets\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.iterations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmMbc_I-Z2TD"
      },
      "source": [
        "Now we can wrap this shapes dataset in a PyTorch datset and use a dataloader to load batches of these images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaB2F-LIaesa"
      },
      "source": [
        "dataset = PytorchDaset(shapes_dataset, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av3uhYXjbj5m"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    for i, t in batch:\n",
        "        images.append(i)\n",
        "        targets.append(t)\n",
        "    return images, targets\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=4, num_workers=0,\n",
        "    collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pksjZJKYd7hE"
      },
      "source": [
        "## Replacing model head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DiJZbBWeNUD"
      },
      "source": [
        "To do transfer learning, we have to replace the top part of the model for one with our amount of classes (4 counting background). We can just, and change the `FastRCNNPredictor` for a new one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxbSWzL-FtSP"
      },
      "source": [
        "### Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsDHswa6OGzP"
      },
      "source": [
        "Replace the `model.roi_heads.box_predictor` by a new untrained `FastRCNNPredictor`, and use an ADAM optimizer to train only this new module. Add a weight decay of 0.0001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn49Qj61cqlP"
      },
      "source": [
        "num_classes = 4\n",
        "\n",
        "# get the number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = ...\n",
        "\n",
        "model = model.train().to(device)\n",
        "optimizer = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx83fCZ4ZeD-"
      },
      "source": [
        "We can take a look at a generated image and the bounding boxes that we will find. The labels indices are \n",
        "\n",
        "`{\"circle\": 1, \"square\": 2, \"triangle\": 3}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJs-qhHxUlXL"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnDSh7auUm2x"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Complete the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCSkovhcdN7P"
      },
      "source": [
        "model.train()\n",
        "for i, (images, targets) in enumerate(data_loader):\n",
        "    ...\n",
        "    predictions, loss_dict = model(images, targets)\n",
        "    loss = sum(loss for loss in loss_dict.values())\n",
        "    ...\n",
        "\n",
        "    if i%2 == 0:\n",
        "        loss_dict_printable = {k: f\"{v.item():.2f}\" for k, v in loss_dict.items()}\n",
        "        print(f\"[{i}/{len(data_loader)}] loss: {loss_dict_printable}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woqH4HytU6KY"
      },
      "source": [
        "Now we can evaluate the model and check how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZpOV7yhu0EE"
      },
      "source": [
        "model = model.eval()\n",
        "image, target = dataset[0] # Random image\n",
        "with torch.no_grad():\n",
        "    detections, loss_dict = model([image], targets=None)\n",
        "detections = detections[0]\n",
        "iou_threshold = 0.2\n",
        "score_threshold = 0.4\n",
        "keep_idx = torchvision.ops.nms(detections[\"boxes\"], detections[\"scores\"], iou_threshold)\n",
        "boxes = [b for i, b in enumerate(detections[\"boxes\"]) if i in keep_idx]\n",
        "scores = [s for i, s in enumerate(detections[\"scores\"]) if i in keep_idx]\n",
        "labels = [l for i, l in enumerate(detections[\"labels\"]) if i in keep_idx]\n",
        "im = to_pil_image(image.cpu())\n",
        "draw = ImageDraw.Draw(im)\n",
        "idx2label = {v: k for k, v in shapes_dataset.label2idx.items()}\n",
        "for box, score, label in zip(boxes, scores, labels):\n",
        "    if score > score_threshold:\n",
        "        coords = box.cpu().tolist()\n",
        "        draw.rectangle(coords)\n",
        "        text = f\"{idx2label[label.item()]} {score*100:.2f}%\"\n",
        "        draw.text([coords[0], coords[1]-15], text)\n",
        "\n",
        "show_image(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkDZWkqLVDEl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKS6oFO2Ty5v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}