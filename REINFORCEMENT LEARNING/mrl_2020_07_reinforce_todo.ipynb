{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrl_2020_07_reinforce_todo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0spTu6Dawi2S"
      },
      "source": [
        "# Basic Policy Gradients (REINFORCE) example in PyTorch\n",
        "\n",
        "This notebook is an adaptation from the [PyTorch REINFORCE tutorial](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py).\n",
        "\n",
        "**Lab exercise prepared by [Víctor Campos](https://imatge.upc.edu/web/people/victor-campos), and adapted by [Juan José Nieto](https://www.linkedin.com/in/juan-jose-nieto-salas/) and [Xavier Giro-i-Nieto](https://imatge.upc.edu/web/people/xavier-giro) for the [Postgraduate course in Artificial Intelligence with Deep Learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2020).**\n",
        "\n",
        "![Víctor Campos](https://imatge.upc.edu/web/sites/default/files/styles/medium/public/users/vcampos//photo.jpg?itok=eCtqXNX9)\n",
        "![Juan José Nieto](https://media-exp1.licdn.com/dms/image/C5603AQHLMgUe1Jvx-g/profile-displayphoto-shrink_200_200/0?e=1593648000&v=beta&t=AhGwoXIDMNNQj_2P6pFbp7RwD39PhpmpOU8OvOdRqC4)\n",
        "![Xavier Giro-i-Nieto](https://telecombcn-dl.github.io/2019-dlcv/img/instructors/XavierGiro-160x160.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIYC7IeEwrlt"
      },
      "source": [
        "## Installing dependencies\n",
        "\n",
        "We will use OpenAI Gym to simulate the environment, which might not be installed by default. We also need to install some dependencies for visualization purposes (this may take a while)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm4IZndds2u2"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzOcEmlJw6gb"
      },
      "source": [
        "## Setting up the environment\n",
        "\n",
        "We will need some tricks to visualize the simulations in the browser, as simply calling env.render() will not work in this notebook ([source](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=Jyb2Ujuozfi2&forceEdit=true&offline=true&sandboxMode=true))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cElJ0d4PsX5X"
      },
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHiHbaL85e1k"
      },
      "source": [
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DmTRRyhmgYt"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnf0NPZ_ss8u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "404c5dbb-5824-4ca1-daed-950958d89e01"
      },
      "source": [
        "print('PyTorch version: ', torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version:  1.5.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUlPxFV1xtw4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "75f3daf2-f07e-45ed-c424-c7aab87400a8"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_b9tRuVxwP3"
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBKofZ4KrG8Z"
      },
      "source": [
        "## Visualize a random policy in the environment\n",
        "\n",
        "Our goal is to train an agent that is capable of solving the CartPole problem, where a pole is attached to a cart moving along a horizontal track. The agent can interact with the environment by applying a force (+1/-1) to the cart. The episode is terminated whenever the pole is more than 15 degrees from vertical or the cart goes out of bounds in the horizontal axis. The agent receives +1 reward for each timestep under the desired conditions.\n",
        "\n",
        "We can visualize what a random policy would do in this environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH913G8v01dn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "93ede3bc-2f80-4b9d-c55e-d6e82b0b225a"
      },
      "source": [
        "# Let's generate a random trajectory...\n",
        "env = wrap_env(gym.make(\"CartPole-v1\"))\n",
        "ob, done, total_rew = env.reset(), False, 0\n",
        "while not done:\n",
        "  env.render()\n",
        "  ac = env.action_space.sample()\n",
        "  ob, rew, done, info = env.step(ac)\n",
        "  total_rew += rew\n",
        "print('Cumulative reward:', total_rew)\n",
        "  \n",
        "# ... and visualize it!\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cumulative reward: 13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeZXG6VHygrD"
      },
      "source": [
        "## Create the model\n",
        "\n",
        "Now we will define our policy, parameterized by a feedforward neural network.\n",
        "\n",
        "**Exercise #1.** Implement the policy as an MLP with a hidden layer of 128 neurons with a ReLU activation and a Softmax output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKr_lu19yc3B"
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, inputs, outputs):\n",
        "        super(Policy, self).__init__()\n",
        "        # TODO\n",
        "        self.affine1 = nn.TODO\n",
        "        self.affine2 = nn.TODO\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        x = self.TODO\n",
        "        x = F.TODO\n",
        "        action_scores = self.TODO\n",
        "        return F.softmax(action_scores, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T02F2uyndg5"
      },
      "source": [
        "## Functions for collecting experience and updating the policy\n",
        "\n",
        "**Exercise #2.** Use the policy to predict a distribution of probabilities across actions.\n",
        "\n",
        "**Exercise #3.** Compute the return from the rewards collected by the policy.\n",
        "\n",
        "**Exercise #4.** Complete the loss computation using the returns and the log probs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcXYBZNznd96"
      },
      "source": [
        "def select_action(policy, state):\n",
        "    # Convert state into PyTorch tensor\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    # TODO: Compute action probabilities\n",
        "    probs = TODO\n",
        "    # Sample action\n",
        "    m = torch.distributions.Categorical(probs)\n",
        "    action = m.sample()\n",
        "    # Bookkeeping\n",
        "    policy.saved_log_probs.append(m.log_prob(action))\n",
        "    return action.item()\n",
        "\n",
        "\n",
        "def train(policy, optimizer):\n",
        "    R = 0\n",
        "    policy_loss = []\n",
        "    returns = []\n",
        "    # Compute the returns by reading the rewards vector backwards\n",
        "    for r in policy.rewards[::-1]:\n",
        "        # TODO: Complete the computation of the return using gamma\n",
        "        R = r + TODO\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns)\n",
        "    # Normalize returns (this usually accelerates convergence)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
        "        # TODO: Complete the 'loss' computation using the returns and the log probs.\n",
        "        policy_loss.append(TODO)\n",
        "    # Update policy: \n",
        "    #  (1) reset optimizer grads\n",
        "    optimizer.zero_grad()\n",
        "    #  (2) compute surrogate policy gradients loss\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    #  (3) SGD step\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr4pSznznosA"
      },
      "source": [
        "## Training the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tJ7RcgfnxPa"
      },
      "source": [
        "# Hyperparameters\n",
        "env_name = 'CartPole-v1'\n",
        "gamma = 0.99  # discount factor\n",
        "seed = 543  # random seed\n",
        "log_interval = 10  # controls how often we log progress\n",
        "max_ep_len = 1000  # maximum episode length\n",
        "num_episodes = 1500  # number of episodes to train on"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLb1oPuGoJpJ"
      },
      "source": [
        "# Create environment\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Fix random seed (for reproducibility)\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AGdVUq4nrWg"
      },
      "source": [
        "# Create policy and optimizer\n",
        "n_inputs = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "policy = Policy(n_inputs, n_actions)\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-2)\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "\n",
        "# Training loop\n",
        "print(\"Target reward: {}\".format(env.spec.reward_threshold))\n",
        "running_reward = 10\n",
        "ep_rew_history = []\n",
        "for i_episode in range(num_episodes):\n",
        "    # Collect experience\n",
        "    state, ep_reward = env.reset(), 0\n",
        "    for t in range(max_ep_len):  # Don't infinite loop while learning\n",
        "        \n",
        "        action = select_action(policy, state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        policy.rewards.append(reward)\n",
        "        ep_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update running reward\n",
        "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "    \n",
        "    # Perform training step\n",
        "    train(policy, optimizer)\n",
        "    ep_rew_history.append((i_episode, ep_reward))\n",
        "    if i_episode % log_interval == 0:\n",
        "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "              i_episode, ep_reward, running_reward))\n",
        "    if running_reward > env.spec.reward_threshold:\n",
        "        print(\"Solved!\")\n",
        "        break\n",
        "\n",
        "print(\"Finished training! Running reward is now {:.2f} and \"\n",
        "      \"the last episode runs to {} time steps!\".format(running_reward, t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv90goqpVguY"
      },
      "source": [
        "plt.plot([x[0] for x in ep_rew_history], [x[1] for x in ep_rew_history])\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cESm8xBnFj-"
      },
      "source": [
        "## Visualize trained policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F97G8ZBUyPAN"
      },
      "source": [
        "test_env = wrap_env(gym.make(env_name))\n",
        "state, ep_reward, done = test_env.reset(), 0, False\n",
        "while not done:\n",
        "    test_env.render()\n",
        "    action = select_action(policy, state)\n",
        "    state, reward, done, _ = test_env.step(action)\n",
        "    ep_reward += reward\n",
        "print(\"Cumulative reward: {}\".format(ep_reward))\n",
        "\n",
        "test_env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}